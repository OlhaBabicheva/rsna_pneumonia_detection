{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eaf9f8a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-12T09:35:34.592578Z",
     "iopub.status.busy": "2023-10-12T09:35:34.592266Z",
     "iopub.status.idle": "2023-10-12T09:35:38.662650Z",
     "shell.execute_reply": "2023-10-12T09:35:38.661715Z"
    },
    "papermill": {
     "duration": 4.076737,
     "end_time": "2023-10-12T09:35:38.664781",
     "exception": false,
     "start_time": "2023-10-12T09:35:34.588044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02782894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T09:35:38.670866Z",
     "iopub.status.busy": "2023-10-12T09:35:38.670451Z",
     "iopub.status.idle": "2023-10-12T09:35:38.748642Z",
     "shell.execute_reply": "2023-10-12T09:35:38.747696Z"
    },
    "papermill": {
     "duration": 0.083246,
     "end_time": "2023-10-12T09:35:38.750650",
     "exception": false,
     "start_time": "2023-10-12T09:35:38.667404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_labels.csv\")\n",
    "labels = labels.drop_duplicates(\"patientId\")\n",
    "\n",
    "train_images = \"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_images\"\n",
    "processed_images = \"/kaggle/working/processed\"\n",
    "\n",
    "def img_processing(val_split):\n",
    "    train_val_split = round(labels.shape[0] * val_split)\n",
    "\n",
    "    for parent_dir in [\"train\", \"val\"]:\n",
    "        for label in range(2):\n",
    "            Path(processed_images + f\"/{parent_dir}/{label}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for c, patient_id in enumerate(tqdm(labels.patientId)):\n",
    "        dcm_path = Path(train_images + f\"/{patient_id}\")\n",
    "        dcm_path = dcm_path.with_suffix(\".dcm\")\n",
    "        dcm = pydicom.read_file(dcm_path).pixel_array / 255\n",
    "        dcm_arr = cv2.resize(dcm, (224, 224)).astype(np.float16)\n",
    "\n",
    "        label = labels.Target.iloc[c]\n",
    "        \n",
    "        train_or_val = \"train\" if c < train_val_split else \"val\"\n",
    "        np.save(processed_images + f\"/{train_or_val}/{str(label)}/{patient_id}\", dcm_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781f8946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T09:35:38.756247Z",
     "iopub.status.busy": "2023-10-12T09:35:38.755962Z",
     "iopub.status.idle": "2023-10-12T09:41:49.837480Z",
     "shell.execute_reply": "2023-10-12T09:41:49.836508Z"
    },
    "papermill": {
     "duration": 371.086379,
     "end_time": "2023-10-12T09:41:49.839335",
     "exception": false,
     "start_time": "2023-10-12T09:35:38.752956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5a26be52ff4ebe8b5c4b661e2700cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_processing(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f049f5ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T09:41:49.845399Z",
     "iopub.status.busy": "2023-10-12T09:41:49.845150Z",
     "iopub.status.idle": "2023-10-12T09:41:49.854523Z",
     "shell.execute_reply": "2023-10-12T09:41:49.853779Z"
    },
    "papermill": {
     "duration": 0.014155,
     "end_time": "2023-10-12T09:41:49.856150",
     "exception": false,
     "start_time": "2023-10-12T09:41:49.841995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VGG16 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, in_channels: int = 1, num_classes: int = 2):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_layers = self.create_conv_layers(VGG16)\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    def create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        for x in architecture:\n",
    "            if type(x) == int:\n",
    "                out_channels = x\n",
    "                layers += [nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=out_channels,\n",
    "                                     kernel_size=(3,3), \n",
    "                                     stride=(1,1), \n",
    "                                     padding=(1,1)),\n",
    "                            nn.BatchNorm2d(x),\n",
    "                            nn.ReLU()]\n",
    "                in_channels = x\n",
    "            elif x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def test():\n",
    "    batch_size, in_channels, img_size = 64, 1, 224\n",
    "    model = VGGNet(1, 2)\n",
    "    x = torch.randn(batch_size, in_channels, img_size, img_size)\n",
    "    print(model(x).shape)\n",
    "    assert model(x).shape == (batch_size, 2), \"Generator test failed\"\n",
    "    print(\"Success, tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38019c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T09:41:49.861751Z",
     "iopub.status.busy": "2023-10-12T09:41:49.861181Z",
     "iopub.status.idle": "2023-10-12T09:43:03.277977Z",
     "shell.execute_reply": "2023-10-12T09:43:03.277033Z"
    },
    "papermill": {
     "duration": 73.423658,
     "end_time": "2023-10-12T09:43:03.281927",
     "exception": false,
     "start_time": "2023-10-12T09:41:49.858269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n",
      "Success, tests passed!\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6c8fc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T09:43:03.287686Z",
     "iopub.status.busy": "2023-10-12T09:43:03.287396Z",
     "iopub.status.idle": "2023-10-12T09:43:03.292065Z",
     "shell.execute_reply": "2023-10-12T09:43:03.291169Z"
    },
    "papermill": {
     "duration": 0.009369,
     "end_time": "2023-10-12T09:43:03.293627",
     "exception": false,
     "start_time": "2023-10-12T09:43:03.284258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_file(path: str):\n",
    "    return np.load(path).astype(np.float32)\n",
    "\n",
    "def checkpoint(model, filename: str):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "def resume(model, filename: str):\n",
    "    model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7cbdbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T09:43:03.299410Z",
     "iopub.status.busy": "2023-10-12T09:43:03.298938Z",
     "iopub.status.idle": "2023-10-12T09:43:03.435594Z",
     "shell.execute_reply": "2023-10-12T09:43:03.434789Z"
    },
    "papermill": {
     "duration": 0.141547,
     "end_time": "2023-10-12T09:43:03.437503",
     "exception": false,
     "start_time": "2023-10-12T09:43:03.295956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters\n",
    "in_dim = 1\n",
    "out_dim = 2\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "num_epochs = 200\n",
    "e_stop_thresh = 10\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(0.49, 0.248),\n",
    "                                    transforms.RandomAffine(degrees=(-5, 5), translate=(0, 0.05), scale=(0.9, 1.1)),\n",
    "                                    transforms.RandomResizedCrop((224, 224), scale=(0.35, 1), antialias=True)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.49], [0.248]),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.DatasetFolder(\n",
    "    f\"{processed_images}/train/\",\n",
    "    loader=load_file, \n",
    "    extensions=\"npy\", \n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = torchvision.datasets.DatasetFolder(\n",
    "    f\"{processed_images}/val/\",\n",
    "    loader=load_file, \n",
    "    extensions=\"npy\", \n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41339e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T09:43:03.443553Z",
     "iopub.status.busy": "2023-10-12T09:43:03.443169Z",
     "iopub.status.idle": "2023-10-12T09:43:07.647020Z",
     "shell.execute_reply": "2023-10-12T09:43:07.646050Z"
    },
    "papermill": {
     "duration": 4.209141,
     "end_time": "2023-10-12T09:43:07.649087",
     "exception": false,
     "start_time": "2023-10-12T09:43:03.439946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VGGNet(in_dim, out_dim).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "def train_val(train_loader, val_loader, model):\n",
    "    size = len(train_loader.dataset)\n",
    "    best_acc = -1\n",
    "    best_epoch = -1\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print('-'*15)\n",
    "\n",
    "        for batch, (X_train, y_train) in enumerate(train_loader):\n",
    "            X_train = X_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "\n",
    "            # Compute predictions and loss\n",
    "            pred = model(X_train)\n",
    "            loss = loss_fn(pred, y_train)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Running training accuracy\n",
    "            _, prediction = pred.max(1)\n",
    "            \n",
    "            loss, current = loss.item(), (batch + 1) * len(X_train)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "            n_correct = (prediction == y_train).sum()\n",
    "            training_acc = n_correct/X_train.shape[0]\n",
    "            print(f\"training accuracy: {training_acc.item()*100}%\")\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val, y_val = next(val_loader)\n",
    "            X_val = X_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            pred = model(X_val)\n",
    "            _, prediction = pred.max(1)\n",
    "            n_correct = (prediction == y_val).sum()\n",
    "            validation_acc = n_correct/X_val.shape[0]\n",
    "            print(f\"validation accuracy: {validation_acc.item()*100}%\")\n",
    "            print('-'*15)\n",
    "\n",
    "            if validation_acc > best_acc:\n",
    "                best_acc = validation_acc\n",
    "                best_epoch = epoch\n",
    "                checkpoint(model, \"best_model.pth\")\n",
    "            elif epoch - best_epoch > e_stop_thresh:\n",
    "                print(f\"Early stopped training at epoch {epoch+1}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d7ec6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T09:43:07.655435Z",
     "iopub.status.busy": "2023-10-12T09:43:07.655167Z",
     "iopub.status.idle": "2023-10-12T10:27:25.307062Z",
     "shell.execute_reply": "2023-10-12T10:27:25.305740Z"
    },
    "papermill": {
     "duration": 2657.657188,
     "end_time": "2023-10-12T10:27:25.309062",
     "exception": false,
     "start_time": "2023-10-12T09:43:07.651874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "There are 21347 images in training set and 5337 images in validation set\n",
      "\n",
      "Epoch 1\n",
      "---------------\n",
      "loss: 0.746450  [   64/21347]\n",
      "training accuracy: 31.25%\n",
      "loss: 0.665900  [  128/21347]\n",
      "training accuracy: 65.625%\n",
      "loss: 0.578690  [  192/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.651571  [  256/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.510320  [  320/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.636143  [  384/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.670053  [  448/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.654398  [  512/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.687221  [  576/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.780752  [  640/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.608780  [  704/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.728860  [  768/21347]\n",
      "training accuracy: 59.375%\n",
      "loss: 0.556621  [  832/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.596092  [  896/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.593690  [  960/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.680349  [ 1024/21347]\n",
      "training accuracy: 64.0625%\n",
      "loss: 0.655400  [ 1088/21347]\n",
      "training accuracy: 59.375%\n",
      "loss: 0.554723  [ 1152/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.438057  [ 1216/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.477423  [ 1280/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.623240  [ 1344/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.592972  [ 1408/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.463695  [ 1472/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.468280  [ 1536/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.596767  [ 1600/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.467462  [ 1664/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.542285  [ 1728/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.530592  [ 1792/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.389036  [ 1856/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.433042  [ 1920/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.533697  [ 1984/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.548617  [ 2048/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.505630  [ 2112/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.453074  [ 2176/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.579413  [ 2240/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.501101  [ 2304/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.367635  [ 2368/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.532313  [ 2432/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.654690  [ 2496/21347]\n",
      "training accuracy: 64.0625%\n",
      "loss: 0.468770  [ 2560/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.532454  [ 2624/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.549327  [ 2688/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.542795  [ 2752/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.587126  [ 2816/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.435154  [ 2880/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.456145  [ 2944/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.396107  [ 3008/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.523521  [ 3072/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.537582  [ 3136/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.512554  [ 3200/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.385518  [ 3264/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.584650  [ 3328/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.502394  [ 3392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.522481  [ 3456/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.456231  [ 3520/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.495253  [ 3584/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.486935  [ 3648/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.558572  [ 3712/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.469972  [ 3776/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.497739  [ 3840/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.480705  [ 3904/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.558889  [ 3968/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.484917  [ 4032/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.487220  [ 4096/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.442320  [ 4160/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.464200  [ 4224/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.566484  [ 4288/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.406082  [ 4352/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.435287  [ 4416/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.516467  [ 4480/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.508939  [ 4544/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.529440  [ 4608/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.553821  [ 4672/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.507902  [ 4736/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.463753  [ 4800/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.526996  [ 4864/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.448939  [ 4928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.422321  [ 4992/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.345601  [ 5056/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.440476  [ 5120/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.566714  [ 5184/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.394931  [ 5248/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.484486  [ 5312/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.451856  [ 5376/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.323295  [ 5440/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.454498  [ 5504/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.458672  [ 5568/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.494457  [ 5632/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.421048  [ 5696/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.622168  [ 5760/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.422740  [ 5824/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.592988  [ 5888/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.478766  [ 5952/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.447117  [ 6016/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.436761  [ 6080/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.555125  [ 6144/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.437494  [ 6208/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.395933  [ 6272/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.530990  [ 6336/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.621656  [ 6400/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.401284  [ 6464/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.545859  [ 6528/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.434693  [ 6592/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.367804  [ 6656/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.545068  [ 6720/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.361596  [ 6784/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.558469  [ 6848/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.535421  [ 6912/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.336335  [ 6976/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.381725  [ 7040/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.471305  [ 7104/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.600889  [ 7168/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.576279  [ 7232/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.559411  [ 7296/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.505085  [ 7360/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.623341  [ 7424/21347]\n",
      "training accuracy: 65.625%\n",
      "loss: 0.470541  [ 7488/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.419782  [ 7552/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.441845  [ 7616/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.316668  [ 7680/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.657088  [ 7744/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.472648  [ 7808/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.389191  [ 7872/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.685649  [ 7936/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.782413  [ 8000/21347]\n",
      "training accuracy: 65.625%\n",
      "loss: 0.564677  [ 8064/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.351665  [ 8128/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.490213  [ 8192/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.468482  [ 8256/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.423212  [ 8320/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.518629  [ 8384/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.401733  [ 8448/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.478281  [ 8512/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.361020  [ 8576/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.428511  [ 8640/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.437930  [ 8704/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.570087  [ 8768/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.428281  [ 8832/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.587128  [ 8896/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.587094  [ 8960/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.413562  [ 9024/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.483697  [ 9088/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.534693  [ 9152/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.406553  [ 9216/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.522987  [ 9280/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.487747  [ 9344/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.353226  [ 9408/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.495069  [ 9472/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.526612  [ 9536/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.364681  [ 9600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.407133  [ 9664/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.532747  [ 9728/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.609450  [ 9792/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.675007  [ 9856/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.569829  [ 9920/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.347450  [ 9984/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.430582  [10048/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.468027  [10112/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.559672  [10176/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.423585  [10240/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.509126  [10304/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.460553  [10368/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.461555  [10432/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.401040  [10496/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.455501  [10560/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.424426  [10624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.433585  [10688/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.461975  [10752/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.553537  [10816/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.589431  [10880/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.371580  [10944/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.543904  [11008/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.416866  [11072/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.536802  [11136/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.509085  [11200/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.393673  [11264/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.420951  [11328/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.496423  [11392/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.553328  [11456/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.463685  [11520/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.415507  [11584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.472237  [11648/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.541430  [11712/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.406501  [11776/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.454433  [11840/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.381119  [11904/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.480163  [11968/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.519391  [12032/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.369231  [12096/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.382031  [12160/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.483820  [12224/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.487154  [12288/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.559410  [12352/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.432826  [12416/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.619645  [12480/21347]\n",
      "training accuracy: 60.9375%\n",
      "loss: 0.464497  [12544/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.486466  [12608/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.498480  [12672/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.462866  [12736/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.423283  [12800/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.539990  [12864/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.411780  [12928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.428905  [12992/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.382008  [13056/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.437489  [13120/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.500342  [13184/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.551323  [13248/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.464626  [13312/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.389074  [13376/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.390014  [13440/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.441039  [13504/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.406598  [13568/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.310572  [13632/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.454913  [13696/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.365741  [13760/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.505958  [13824/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.439474  [13888/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.369801  [13952/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.381220  [14016/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.722472  [14080/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.275368  [14144/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.360223  [14208/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.542314  [14272/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.361943  [14336/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.566013  [14400/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.308954  [14464/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.396779  [14528/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.491849  [14592/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.366831  [14656/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.455546  [14720/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.378098  [14784/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.457496  [14848/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.415701  [14912/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.455128  [14976/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.414799  [15040/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.402504  [15104/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.378468  [15168/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.450363  [15232/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.371845  [15296/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.383828  [15360/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.534443  [15424/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.463881  [15488/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.532292  [15552/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.370937  [15616/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.675343  [15680/21347]\n",
      "training accuracy: 64.0625%\n",
      "loss: 0.512005  [15744/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.429235  [15808/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.416650  [15872/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.481580  [15936/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.468615  [16000/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.532034  [16064/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.493825  [16128/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.440306  [16192/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.401396  [16256/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.429222  [16320/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.501341  [16384/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.543690  [16448/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.419543  [16512/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.444672  [16576/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.499862  [16640/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.417523  [16704/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.378308  [16768/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.446352  [16832/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.537625  [16896/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.398900  [16960/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.312262  [17024/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.569340  [17088/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.585390  [17152/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.483837  [17216/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.494611  [17280/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.510367  [17344/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.578764  [17408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.489510  [17472/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.342443  [17536/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.440364  [17600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.486856  [17664/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.452048  [17728/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.491561  [17792/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.559788  [17856/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.482282  [17920/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.494894  [17984/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.426639  [18048/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.501715  [18112/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.529258  [18176/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.416180  [18240/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.503047  [18304/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.411791  [18368/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.476566  [18432/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.561921  [18496/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.387476  [18560/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.459532  [18624/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.525060  [18688/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.387511  [18752/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.409467  [18816/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.443091  [18880/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.488101  [18944/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.409147  [19008/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.539983  [19072/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.480304  [19136/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.402451  [19200/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.440557  [19264/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.495383  [19328/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.432276  [19392/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.348593  [19456/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.395122  [19520/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.426206  [19584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.476242  [19648/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.557544  [19712/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.550422  [19776/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.339651  [19840/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.419051  [19904/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.473939  [19968/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.384713  [20032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.491014  [20096/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.398908  [20160/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.426973  [20224/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.381068  [20288/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.382802  [20352/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.444775  [20416/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.454661  [20480/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.481577  [20544/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.607049  [20608/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.472047  [20672/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.274085  [20736/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.391966  [20800/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.465294  [20864/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.511434  [20928/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.425797  [20992/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.557049  [21056/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.384949  [21120/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.469481  [21184/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.524016  [21248/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.421472  [21312/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.344618  [11690/21347]\n",
      "training accuracy: 82.85714387893677%\n",
      "validation accuracy: 92.1875%\n",
      "---------------\n",
      "Epoch 2\n",
      "---------------\n",
      "loss: 0.431784  [   64/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.690869  [  128/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.518691  [  192/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.453295  [  256/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.489877  [  320/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.486502  [  384/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.493000  [  448/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.446207  [  512/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.400285  [  576/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.576010  [  640/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.480031  [  704/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.495708  [  768/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.534917  [  832/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.382394  [  896/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.502209  [  960/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.400232  [ 1024/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.595513  [ 1088/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.481727  [ 1152/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.501226  [ 1216/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.519471  [ 1280/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.510247  [ 1344/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.345647  [ 1408/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.290492  [ 1472/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.589984  [ 1536/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.550399  [ 1600/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.623989  [ 1664/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.499300  [ 1728/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.518469  [ 1792/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.578619  [ 1856/21347]\n",
      "training accuracy: 62.5%\n",
      "loss: 0.489865  [ 1920/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.415915  [ 1984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.405365  [ 2048/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.535087  [ 2112/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.567276  [ 2176/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.433584  [ 2240/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.650609  [ 2304/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.374488  [ 2368/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.416609  [ 2432/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.433738  [ 2496/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.485457  [ 2560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.468955  [ 2624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.486013  [ 2688/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.447929  [ 2752/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.397167  [ 2816/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.436635  [ 2880/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.533267  [ 2944/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.524921  [ 3008/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.457779  [ 3072/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.682269  [ 3136/21347]\n",
      "training accuracy: 60.9375%\n",
      "loss: 0.411269  [ 3200/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.532588  [ 3264/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.544403  [ 3328/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.510071  [ 3392/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.380290  [ 3456/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.454781  [ 3520/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.409577  [ 3584/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.437455  [ 3648/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.402240  [ 3712/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.530859  [ 3776/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.443006  [ 3840/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.456419  [ 3904/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.530196  [ 3968/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.443945  [ 4032/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.396566  [ 4096/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.472127  [ 4160/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.547861  [ 4224/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.463768  [ 4288/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.441040  [ 4352/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.472487  [ 4416/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.458397  [ 4480/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.532246  [ 4544/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.617916  [ 4608/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.466696  [ 4672/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.415913  [ 4736/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.404645  [ 4800/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.366889  [ 4864/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.402521  [ 4928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.466757  [ 4992/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.422294  [ 5056/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.585146  [ 5120/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.484058  [ 5184/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.246320  [ 5248/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.469393  [ 5312/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.280567  [ 5376/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.252639  [ 5440/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.370110  [ 5504/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.448285  [ 5568/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.375185  [ 5632/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.385709  [ 5696/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.509802  [ 5760/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.326085  [ 5824/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.384852  [ 5888/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.318802  [ 5952/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.444981  [ 6016/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.552777  [ 6080/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.526543  [ 6144/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.407311  [ 6208/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.496630  [ 6272/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.406893  [ 6336/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.451361  [ 6400/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.390994  [ 6464/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.471091  [ 6528/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.524002  [ 6592/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.481957  [ 6656/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.495176  [ 6720/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.396912  [ 6784/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.440807  [ 6848/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.412652  [ 6912/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.354951  [ 6976/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.644519  [ 7040/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.464727  [ 7104/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.473468  [ 7168/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.410649  [ 7232/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.435226  [ 7296/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.407260  [ 7360/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.410129  [ 7424/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.518511  [ 7488/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.354265  [ 7552/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.511193  [ 7616/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.462447  [ 7680/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.391973  [ 7744/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.488115  [ 7808/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.447145  [ 7872/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.389271  [ 7936/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.418735  [ 8000/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.441177  [ 8064/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.441837  [ 8128/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.453438  [ 8192/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.429384  [ 8256/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.450924  [ 8320/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.383426  [ 8384/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.568762  [ 8448/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.417905  [ 8512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.307076  [ 8576/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.532236  [ 8640/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.527596  [ 8704/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.431754  [ 8768/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.374443  [ 8832/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.380745  [ 8896/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.420928  [ 8960/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.517345  [ 9024/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.381117  [ 9088/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.456414  [ 9152/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.315655  [ 9216/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.521843  [ 9280/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.408806  [ 9344/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.328935  [ 9408/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.431755  [ 9472/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.426800  [ 9536/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.397082  [ 9600/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.371131  [ 9664/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.353898  [ 9728/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.330916  [ 9792/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.430216  [ 9856/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.466662  [ 9920/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.389633  [ 9984/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.371267  [10048/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.701523  [10112/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.443125  [10176/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.466129  [10240/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.474521  [10304/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.459535  [10368/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.518661  [10432/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.435982  [10496/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.352868  [10560/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.328687  [10624/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.450890  [10688/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.580594  [10752/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.445110  [10816/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.426255  [10880/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.458970  [10944/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.433532  [11008/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.364184  [11072/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.455502  [11136/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.420606  [11200/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.579030  [11264/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.515771  [11328/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.381062  [11392/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.513245  [11456/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.468878  [11520/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.545046  [11584/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.421692  [11648/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.460186  [11712/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.496565  [11776/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.412594  [11840/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.492833  [11904/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.454029  [11968/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.455231  [12032/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.428823  [12096/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.509701  [12160/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.435396  [12224/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.325708  [12288/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.419899  [12352/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.295634  [12416/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.487228  [12480/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.429135  [12544/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.441055  [12608/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.562056  [12672/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.526489  [12736/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.399651  [12800/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.454992  [12864/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.484594  [12928/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.495817  [12992/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.463517  [13056/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.609940  [13120/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.558162  [13184/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.376935  [13248/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.348704  [13312/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.508240  [13376/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.578753  [13440/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.495297  [13504/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.456389  [13568/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.527357  [13632/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.375882  [13696/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.434635  [13760/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.506780  [13824/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.355240  [13888/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.524942  [13952/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.375511  [14016/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.375637  [14080/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.500870  [14144/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.359022  [14208/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.388219  [14272/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.466914  [14336/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.469205  [14400/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.432539  [14464/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.271727  [14528/21347]\n",
      "training accuracy: 95.3125%\n",
      "loss: 0.319039  [14592/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.383357  [14656/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.382244  [14720/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.642022  [14784/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.423970  [14848/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.519933  [14912/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.405847  [14976/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.484235  [15040/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.415878  [15104/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.443452  [15168/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.300600  [15232/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.623068  [15296/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.339968  [15360/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.521625  [15424/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.361784  [15488/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.326442  [15552/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.389333  [15616/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.509684  [15680/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.423212  [15744/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.510881  [15808/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.391551  [15872/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.477320  [15936/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.524873  [16000/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.379664  [16064/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.497910  [16128/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.391226  [16192/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.303910  [16256/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.547167  [16320/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.331941  [16384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.423243  [16448/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.519301  [16512/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.479170  [16576/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.412530  [16640/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.407209  [16704/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.388439  [16768/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.449984  [16832/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.384910  [16896/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.501480  [16960/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.540424  [17024/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.579617  [17088/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.509530  [17152/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.565337  [17216/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.452011  [17280/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.405277  [17344/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.360106  [17408/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.338837  [17472/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.589853  [17536/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.441257  [17600/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.470511  [17664/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.436667  [17728/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.431685  [17792/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.408504  [17856/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.496781  [17920/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.501338  [17984/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.389942  [18048/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.395347  [18112/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.465890  [18176/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.495611  [18240/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.451953  [18304/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.531732  [18368/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.343208  [18432/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.504959  [18496/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.510603  [18560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.536899  [18624/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.497057  [18688/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.546759  [18752/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.380815  [18816/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.515271  [18880/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.467508  [18944/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.469472  [19008/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.431542  [19072/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.394757  [19136/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.456040  [19200/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.501372  [19264/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.370750  [19328/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.439228  [19392/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.434764  [19456/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.645464  [19520/21347]\n",
      "training accuracy: 65.625%\n",
      "loss: 0.397061  [19584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.435408  [19648/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.416932  [19712/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.499784  [19776/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.341521  [19840/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.401428  [19904/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.358949  [19968/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.543528  [20032/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.551313  [20096/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.417112  [20160/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.349409  [20224/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.464151  [20288/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.437594  [20352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.437235  [20416/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.617956  [20480/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.378716  [20544/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.474280  [20608/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.548997  [20672/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.405409  [20736/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.432106  [20800/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.380243  [20864/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.444745  [20928/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.412911  [20992/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.361824  [21056/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.495901  [21120/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.446225  [21184/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.416961  [21248/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.495882  [21312/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.366514  [11690/21347]\n",
      "training accuracy: 80.0000011920929%\n",
      "validation accuracy: 96.875%\n",
      "---------------\n",
      "Epoch 3\n",
      "---------------\n",
      "loss: 0.463724  [   64/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.388975  [  128/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.419739  [  192/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.439367  [  256/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.414522  [  320/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.292741  [  384/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.528205  [  448/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.336543  [  512/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.380600  [  576/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.482617  [  640/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.406647  [  704/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.390911  [  768/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.358243  [  832/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.407058  [  896/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.327000  [  960/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.411813  [ 1024/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.374308  [ 1088/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.513193  [ 1152/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.480922  [ 1216/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.451398  [ 1280/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.491317  [ 1344/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.469220  [ 1408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.431713  [ 1472/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.401681  [ 1536/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.475783  [ 1600/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.473993  [ 1664/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.408882  [ 1728/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.433372  [ 1792/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.417547  [ 1856/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.468973  [ 1920/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.479542  [ 1984/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.429878  [ 2048/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.364395  [ 2112/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.480172  [ 2176/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.374419  [ 2240/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.461401  [ 2304/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.472556  [ 2368/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.428839  [ 2432/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.406108  [ 2496/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.395195  [ 2560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.358151  [ 2624/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.318606  [ 2688/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.401344  [ 2752/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.433604  [ 2816/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.370579  [ 2880/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.448844  [ 2944/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.401076  [ 3008/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.469283  [ 3072/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.384637  [ 3136/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.363503  [ 3200/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.336955  [ 3264/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.300677  [ 3328/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.379187  [ 3392/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.508371  [ 3456/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.651326  [ 3520/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.442301  [ 3584/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.447828  [ 3648/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.532003  [ 3712/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.439757  [ 3776/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.508323  [ 3840/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.459231  [ 3904/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.411143  [ 3968/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.415287  [ 4032/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.469242  [ 4096/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.365086  [ 4160/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.389134  [ 4224/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.549052  [ 4288/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.412694  [ 4352/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.458774  [ 4416/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.476880  [ 4480/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.516363  [ 4544/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.440747  [ 4608/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.426530  [ 4672/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.506578  [ 4736/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.347258  [ 4800/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.502004  [ 4864/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.414898  [ 4928/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.472646  [ 4992/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.319948  [ 5056/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.416618  [ 5120/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.372152  [ 5184/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.436249  [ 5248/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.335620  [ 5312/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.375505  [ 5376/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.501378  [ 5440/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.442736  [ 5504/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.536895  [ 5568/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.384663  [ 5632/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.424595  [ 5696/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.531755  [ 5760/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.439011  [ 5824/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.362660  [ 5888/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.352154  [ 5952/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.454487  [ 6016/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.392670  [ 6080/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.400187  [ 6144/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.473317  [ 6208/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.510697  [ 6272/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.630363  [ 6336/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.323026  [ 6400/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.574169  [ 6464/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.490790  [ 6528/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.409115  [ 6592/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.481199  [ 6656/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.332959  [ 6720/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.504065  [ 6784/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.498008  [ 6848/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.435819  [ 6912/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.360348  [ 6976/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.391032  [ 7040/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.531058  [ 7104/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.422626  [ 7168/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.451924  [ 7232/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.563553  [ 7296/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.413315  [ 7360/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.426388  [ 7424/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.378022  [ 7488/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.443947  [ 7552/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.597559  [ 7616/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.474075  [ 7680/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.553307  [ 7744/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.341604  [ 7808/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.428479  [ 7872/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.438692  [ 7936/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.410057  [ 8000/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.374006  [ 8064/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.443434  [ 8128/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.459638  [ 8192/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.474753  [ 8256/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.344008  [ 8320/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.445508  [ 8384/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.476718  [ 8448/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.436190  [ 8512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.444503  [ 8576/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.437653  [ 8640/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.326981  [ 8704/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.448784  [ 8768/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.411492  [ 8832/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.480513  [ 8896/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.403573  [ 8960/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.359091  [ 9024/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.592785  [ 9088/21347]\n",
      "training accuracy: 64.0625%\n",
      "loss: 0.385838  [ 9152/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.389667  [ 9216/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.536157  [ 9280/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.438809  [ 9344/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.369459  [ 9408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.492665  [ 9472/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.337603  [ 9536/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.452562  [ 9600/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.332476  [ 9664/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.507813  [ 9728/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.354633  [ 9792/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.493317  [ 9856/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.519065  [ 9920/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.559426  [ 9984/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.406241  [10048/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.402227  [10112/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.444336  [10176/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.390998  [10240/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.369142  [10304/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.522412  [10368/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.357543  [10432/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.382458  [10496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.353394  [10560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.397126  [10624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.330655  [10688/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.501878  [10752/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.516225  [10816/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.387999  [10880/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.430502  [10944/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.310838  [11008/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.408877  [11072/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.536650  [11136/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.443377  [11200/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.412665  [11264/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.288410  [11328/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.445832  [11392/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.369085  [11456/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.428360  [11520/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.279767  [11584/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.499939  [11648/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.457906  [11712/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.351878  [11776/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.379608  [11840/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.379744  [11904/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.480217  [11968/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.397197  [12032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.579658  [12096/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.408789  [12160/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.416286  [12224/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.374273  [12288/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.419352  [12352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.331053  [12416/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.446287  [12480/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.333398  [12544/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.435102  [12608/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.451112  [12672/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.468663  [12736/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.295186  [12800/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.334602  [12864/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.422817  [12928/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.419508  [12992/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.386689  [13056/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.364306  [13120/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.338598  [13184/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.460953  [13248/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.470254  [13312/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.406572  [13376/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.610477  [13440/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.395188  [13504/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.461076  [13568/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.457216  [13632/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.394558  [13696/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.411831  [13760/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.429739  [13824/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.556319  [13888/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.413569  [13952/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.469132  [14016/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.441265  [14080/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.488787  [14144/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.351356  [14208/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.387675  [14272/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.443082  [14336/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.451630  [14400/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.395793  [14464/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.477453  [14528/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.397237  [14592/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.478449  [14656/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.446373  [14720/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.555659  [14784/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.485983  [14848/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.453281  [14912/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.359122  [14976/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.369336  [15040/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.351211  [15104/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.391715  [15168/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.305133  [15232/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.564223  [15296/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.470719  [15360/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.448235  [15424/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.476847  [15488/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.410419  [15552/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.366920  [15616/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.462349  [15680/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.433049  [15744/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.432639  [15808/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.411815  [15872/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.343022  [15936/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.382764  [16000/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.411607  [16064/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.338369  [16128/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.202555  [16192/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.529283  [16256/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.480790  [16320/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.590932  [16384/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.412857  [16448/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.467082  [16512/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.349378  [16576/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.472126  [16640/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.455747  [16704/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.422942  [16768/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.416654  [16832/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.525188  [16896/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.393773  [16960/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.335779  [17024/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.436885  [17088/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.503656  [17152/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.454022  [17216/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.294898  [17280/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.301989  [17344/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.490428  [17408/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.360561  [17472/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.552874  [17536/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.469411  [17600/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.352613  [17664/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.442173  [17728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.379122  [17792/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.381404  [17856/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.463931  [17920/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.502435  [17984/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.457313  [18048/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.529526  [18112/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.421084  [18176/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.453836  [18240/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.442456  [18304/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.383877  [18368/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.519506  [18432/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.440673  [18496/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.462133  [18560/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.400750  [18624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.488806  [18688/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.391339  [18752/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.387368  [18816/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.364001  [18880/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.314316  [18944/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.409543  [19008/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.261476  [19072/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.425500  [19136/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.441690  [19200/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.516128  [19264/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.440308  [19328/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.421511  [19392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.378352  [19456/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.437313  [19520/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.415291  [19584/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.293899  [19648/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.359109  [19712/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.451391  [19776/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.343789  [19840/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.378696  [19904/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.582291  [19968/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.558601  [20032/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.367194  [20096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.640128  [20160/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.528709  [20224/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.417594  [20288/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.505933  [20352/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.391394  [20416/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.423636  [20480/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.421746  [20544/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.487538  [20608/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.388126  [20672/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.355470  [20736/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.394714  [20800/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.511239  [20864/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.432401  [20928/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.439209  [20992/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.358136  [21056/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.415046  [21120/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.435387  [21184/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.469486  [21248/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.504457  [21312/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.516416  [11690/21347]\n",
      "training accuracy: 71.42857313156128%\n",
      "validation accuracy: 92.1875%\n",
      "---------------\n",
      "Epoch 4\n",
      "---------------\n",
      "loss: 0.532634  [   64/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.397454  [  128/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.403542  [  192/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.482717  [  256/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.415678  [  320/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.464231  [  384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.405460  [  448/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.409623  [  512/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.349427  [  576/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.372648  [  640/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.417636  [  704/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.255973  [  768/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.330599  [  832/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.366064  [  896/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.446637  [  960/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.314741  [ 1024/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.472247  [ 1088/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.376168  [ 1152/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.481398  [ 1216/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.414409  [ 1280/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.493785  [ 1344/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.502861  [ 1408/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.553692  [ 1472/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.360790  [ 1536/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.484775  [ 1600/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.481384  [ 1664/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.359881  [ 1728/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.306723  [ 1792/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.431529  [ 1856/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.567936  [ 1920/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.438714  [ 1984/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.386523  [ 2048/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.489499  [ 2112/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.431358  [ 2176/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.449389  [ 2240/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.361437  [ 2304/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.575376  [ 2368/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.448837  [ 2432/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.437494  [ 2496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.485410  [ 2560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.484372  [ 2624/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.440637  [ 2688/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.388971  [ 2752/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.546817  [ 2816/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.390574  [ 2880/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.452316  [ 2944/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.478885  [ 3008/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.475194  [ 3072/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.494708  [ 3136/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.418118  [ 3200/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.418323  [ 3264/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.386590  [ 3328/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.545299  [ 3392/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.508632  [ 3456/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.418077  [ 3520/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.485231  [ 3584/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.521680  [ 3648/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.440941  [ 3712/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.475347  [ 3776/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.426984  [ 3840/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.387462  [ 3904/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.302663  [ 3968/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.369711  [ 4032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.404273  [ 4096/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.351496  [ 4160/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.424637  [ 4224/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.493128  [ 4288/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.295441  [ 4352/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.346297  [ 4416/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.389376  [ 4480/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.412541  [ 4544/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.307645  [ 4608/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.455604  [ 4672/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.466678  [ 4736/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.470478  [ 4800/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.406696  [ 4864/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.400085  [ 4928/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.413602  [ 4992/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.462531  [ 5056/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.451605  [ 5120/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.438658  [ 5184/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.420075  [ 5248/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.476157  [ 5312/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.396589  [ 5376/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.422236  [ 5440/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.494293  [ 5504/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.355451  [ 5568/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.401003  [ 5632/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.334705  [ 5696/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.497423  [ 5760/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.480607  [ 5824/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.447445  [ 5888/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.352180  [ 5952/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.395051  [ 6016/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.486383  [ 6080/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.409843  [ 6144/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.455224  [ 6208/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.368986  [ 6272/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.464143  [ 6336/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.377768  [ 6400/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.410340  [ 6464/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.489034  [ 6528/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.428480  [ 6592/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.533728  [ 6656/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.317757  [ 6720/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.319476  [ 6784/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.311656  [ 6848/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.630012  [ 6912/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.376769  [ 6976/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.413246  [ 7040/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.365473  [ 7104/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.305143  [ 7168/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.443835  [ 7232/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.335224  [ 7296/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.483626  [ 7360/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.396459  [ 7424/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.415181  [ 7488/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.496225  [ 7552/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.527035  [ 7616/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.490930  [ 7680/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.384172  [ 7744/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.426782  [ 7808/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.365784  [ 7872/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.321705  [ 7936/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.544847  [ 8000/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.335288  [ 8064/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.543009  [ 8128/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.509621  [ 8192/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.294684  [ 8256/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.320414  [ 8320/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.453824  [ 8384/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.469654  [ 8448/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.364671  [ 8512/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.305816  [ 8576/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.400345  [ 8640/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.376127  [ 8704/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.599829  [ 8768/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.389911  [ 8832/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.540000  [ 8896/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.529932  [ 8960/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.414193  [ 9024/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.499791  [ 9088/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.454115  [ 9152/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.482699  [ 9216/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.588447  [ 9280/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.466524  [ 9344/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.290139  [ 9408/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.298208  [ 9472/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.452248  [ 9536/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.444924  [ 9600/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.470642  [ 9664/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.477456  [ 9728/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.390011  [ 9792/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.414804  [ 9856/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.419763  [ 9920/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.509393  [ 9984/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.352044  [10048/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.529341  [10112/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.290127  [10176/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.375251  [10240/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.268676  [10304/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.469600  [10368/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.307671  [10432/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.554068  [10496/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.309589  [10560/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.322935  [10624/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.537169  [10688/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.407768  [10752/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.503903  [10816/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.307523  [10880/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.537429  [10944/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.478930  [11008/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.427805  [11072/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.520160  [11136/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.453925  [11200/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.508960  [11264/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.401428  [11328/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.318618  [11392/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.380424  [11456/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.454874  [11520/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.386950  [11584/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.402804  [11648/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.553376  [11712/21347]\n",
      "training accuracy: 65.625%\n",
      "loss: 0.331885  [11776/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.455534  [11840/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.403401  [11904/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.447063  [11968/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.455186  [12032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.360884  [12096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.362761  [12160/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.410056  [12224/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.543078  [12288/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.379861  [12352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.319762  [12416/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.404329  [12480/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.309379  [12544/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.437074  [12608/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.356840  [12672/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.547253  [12736/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.417485  [12800/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.508600  [12864/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.333503  [12928/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.400412  [12992/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.338056  [13056/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.437078  [13120/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.409195  [13184/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.368635  [13248/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.374019  [13312/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.391589  [13376/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.346289  [13440/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.380282  [13504/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.414292  [13568/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.540336  [13632/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.384150  [13696/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.316220  [13760/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.450383  [13824/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.377510  [13888/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.427884  [13952/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.409121  [14016/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.422952  [14080/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.484486  [14144/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.393818  [14208/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.413589  [14272/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.450166  [14336/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.431292  [14400/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.361399  [14464/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.487255  [14528/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.433760  [14592/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.364457  [14656/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.461816  [14720/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.475811  [14784/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.520549  [14848/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.394128  [14912/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.509387  [14976/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.423259  [15040/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.409655  [15104/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.295446  [15168/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.408780  [15232/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.543900  [15296/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.471602  [15360/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.427533  [15424/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.372165  [15488/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.372059  [15552/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.453002  [15616/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.526841  [15680/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.511894  [15744/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.368207  [15808/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.355356  [15872/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.399105  [15936/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.428874  [16000/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.366902  [16064/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.356731  [16128/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.397592  [16192/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.340444  [16256/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.494267  [16320/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.303395  [16384/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.455237  [16448/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.451918  [16512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.433393  [16576/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.454819  [16640/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.452773  [16704/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.445599  [16768/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.450891  [16832/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.317580  [16896/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.412641  [16960/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.331459  [17024/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.489291  [17088/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.320666  [17152/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.438706  [17216/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.422525  [17280/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.498783  [17344/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.335549  [17408/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.274782  [17472/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.446032  [17536/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.577206  [17600/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.403833  [17664/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.421589  [17728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.365520  [17792/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.406018  [17856/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.408998  [17920/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.416960  [17984/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.383047  [18048/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.282153  [18112/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.459712  [18176/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.491401  [18240/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.322339  [18304/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.318572  [18368/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.405526  [18432/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.265976  [18496/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.551474  [18560/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.497303  [18624/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.325219  [18688/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.471742  [18752/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.422282  [18816/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.475726  [18880/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.404911  [18944/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.503767  [19008/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.375459  [19072/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.395088  [19136/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.479129  [19200/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.416922  [19264/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.553251  [19328/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.361515  [19392/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.462384  [19456/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.414105  [19520/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.465988  [19584/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.344454  [19648/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.355095  [19712/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.435705  [19776/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.382324  [19840/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.405423  [19904/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.591581  [19968/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.513890  [20032/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.411747  [20096/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.410410  [20160/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.448165  [20224/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.348931  [20288/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.384057  [20352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.446654  [20416/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.353754  [20480/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.403905  [20544/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.384207  [20608/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.371362  [20672/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.501187  [20736/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.435987  [20800/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.293827  [20864/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.657676  [20928/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.433641  [20992/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.411163  [21056/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.373452  [21120/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.471306  [21184/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.434215  [21248/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.387503  [21312/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.366802  [11690/21347]\n",
      "training accuracy: 85.71428656578064%\n",
      "validation accuracy: 100.0%\n",
      "---------------\n",
      "Epoch 5\n",
      "---------------\n",
      "loss: 0.389478  [   64/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.434474  [  128/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.272143  [  192/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.362465  [  256/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.649796  [  320/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.519911  [  384/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.497324  [  448/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.393341  [  512/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.339306  [  576/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.458108  [  640/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.563599  [  704/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.439614  [  768/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.356159  [  832/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.396621  [  896/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.477384  [  960/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.404782  [ 1024/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.386985  [ 1088/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.360365  [ 1152/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.439314  [ 1216/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.339040  [ 1280/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.421153  [ 1344/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.361215  [ 1408/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.324232  [ 1472/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.593099  [ 1536/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.380026  [ 1600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.423258  [ 1664/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.447750  [ 1728/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.478518  [ 1792/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.245240  [ 1856/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.386659  [ 1920/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.345199  [ 1984/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.465265  [ 2048/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.516806  [ 2112/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.345538  [ 2176/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.443476  [ 2240/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.436512  [ 2304/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.413400  [ 2368/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.304873  [ 2432/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.338305  [ 2496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.439710  [ 2560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.361500  [ 2624/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.388481  [ 2688/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.277463  [ 2752/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.379567  [ 2816/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.282975  [ 2880/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.208312  [ 2944/21347]\n",
      "training accuracy: 96.875%\n",
      "loss: 0.317871  [ 3008/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.364111  [ 3072/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.459554  [ 3136/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.517061  [ 3200/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.349661  [ 3264/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.251371  [ 3328/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.399853  [ 3392/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.289856  [ 3456/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.415894  [ 3520/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.338988  [ 3584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.523572  [ 3648/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.517349  [ 3712/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.394645  [ 3776/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.384732  [ 3840/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.400806  [ 3904/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.420742  [ 3968/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.374765  [ 4032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.396380  [ 4096/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.356949  [ 4160/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.398546  [ 4224/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.571257  [ 4288/21347]\n",
      "training accuracy: 65.625%\n",
      "loss: 0.334063  [ 4352/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.465602  [ 4416/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.447691  [ 4480/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.384931  [ 4544/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.414959  [ 4608/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.434974  [ 4672/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.455888  [ 4736/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.450283  [ 4800/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.520703  [ 4864/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.439499  [ 4928/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.392841  [ 4992/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.311433  [ 5056/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.417481  [ 5120/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.523005  [ 5184/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.487783  [ 5248/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.660952  [ 5312/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.386161  [ 5376/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.389362  [ 5440/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.481619  [ 5504/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.423634  [ 5568/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.439868  [ 5632/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.522845  [ 5696/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.466014  [ 5760/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.372849  [ 5824/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.477240  [ 5888/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.342413  [ 5952/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.311192  [ 6016/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.334429  [ 6080/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.430322  [ 6144/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.350653  [ 6208/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.541114  [ 6272/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.330741  [ 6336/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.370210  [ 6400/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.531690  [ 6464/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.297769  [ 6528/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.333870  [ 6592/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.352689  [ 6656/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.505041  [ 6720/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.389720  [ 6784/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.495466  [ 6848/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.425129  [ 6912/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.505055  [ 6976/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.427279  [ 7040/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.401280  [ 7104/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.396047  [ 7168/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.427048  [ 7232/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.443383  [ 7296/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.441709  [ 7360/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.439750  [ 7424/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.418443  [ 7488/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.488940  [ 7552/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.346070  [ 7616/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.329822  [ 7680/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.338926  [ 7744/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.372926  [ 7808/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.407506  [ 7872/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.275761  [ 7936/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.374930  [ 8000/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.417592  [ 8064/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.406516  [ 8128/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.434158  [ 8192/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.426255  [ 8256/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.392680  [ 8320/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.490361  [ 8384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.525850  [ 8448/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.472639  [ 8512/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.443417  [ 8576/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.482538  [ 8640/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.421288  [ 8704/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.466435  [ 8768/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.460716  [ 8832/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.351689  [ 8896/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.458586  [ 8960/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.588626  [ 9024/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.430526  [ 9088/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.406041  [ 9152/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.432468  [ 9216/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.448666  [ 9280/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.372942  [ 9344/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.481374  [ 9408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.469968  [ 9472/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.337115  [ 9536/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.394703  [ 9600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.549030  [ 9664/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.401016  [ 9728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.554271  [ 9792/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.399216  [ 9856/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.419129  [ 9920/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.366995  [ 9984/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.439107  [10048/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.467742  [10112/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.432611  [10176/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.421228  [10240/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.289403  [10304/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.476906  [10368/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.429503  [10432/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.390783  [10496/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.457962  [10560/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.461835  [10624/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.273509  [10688/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.404661  [10752/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.393927  [10816/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.298732  [10880/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.353593  [10944/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.317909  [11008/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.468618  [11072/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.576093  [11136/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.322453  [11200/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.488906  [11264/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.454993  [11328/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.430432  [11392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.530912  [11456/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.449872  [11520/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.521143  [11584/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.340525  [11648/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.379321  [11712/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.501747  [11776/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.367747  [11840/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.436689  [11904/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.442774  [11968/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.462882  [12032/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.295333  [12096/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.423506  [12160/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.451611  [12224/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.416030  [12288/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.515748  [12352/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.403084  [12416/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.435726  [12480/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.384121  [12544/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.420589  [12608/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.385476  [12672/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.378211  [12736/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.389755  [12800/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.431961  [12864/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.336989  [12928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.465168  [12992/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.516399  [13056/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.599526  [13120/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.361283  [13184/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.520583  [13248/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.388245  [13312/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.517898  [13376/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.378221  [13440/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.441832  [13504/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.477843  [13568/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.419281  [13632/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.352937  [13696/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.460391  [13760/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.291578  [13824/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.384979  [13888/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.422632  [13952/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.436987  [14016/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.448044  [14080/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.370001  [14144/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.442064  [14208/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.394097  [14272/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.446353  [14336/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.351083  [14400/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.367014  [14464/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.366217  [14528/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.266163  [14592/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.399514  [14656/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.492649  [14720/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.502834  [14784/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.527876  [14848/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.413169  [14912/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.408323  [14976/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.490217  [15040/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.297392  [15104/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.375530  [15168/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.298192  [15232/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.423720  [15296/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.458952  [15360/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.398674  [15424/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.285677  [15488/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.336187  [15552/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.344950  [15616/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.522233  [15680/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.441135  [15744/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.420476  [15808/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.297239  [15872/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.402465  [15936/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.505024  [16000/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.393484  [16064/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.429315  [16128/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.431766  [16192/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.412290  [16256/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.509825  [16320/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.362533  [16384/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.487666  [16448/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.398237  [16512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.374375  [16576/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.463874  [16640/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.406756  [16704/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.441937  [16768/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.469453  [16832/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.437095  [16896/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.401062  [16960/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.353613  [17024/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.408835  [17088/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.274566  [17152/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.363640  [17216/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.341227  [17280/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.622589  [17344/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.498499  [17408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.340719  [17472/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.324684  [17536/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.302867  [17600/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.322317  [17664/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.431317  [17728/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.304537  [17792/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.417013  [17856/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.506190  [17920/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.463826  [17984/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.322567  [18048/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.449246  [18112/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.415901  [18176/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.314521  [18240/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.325417  [18304/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.585674  [18368/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.393413  [18432/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.540961  [18496/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.388613  [18560/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.435004  [18624/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.438233  [18688/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.443717  [18752/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.296094  [18816/21347]\n",
      "training accuracy: 95.3125%\n",
      "loss: 0.391914  [18880/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.442998  [18944/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.485617  [19008/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.371229  [19072/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.382976  [19136/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.340441  [19200/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.467076  [19264/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.527698  [19328/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.481493  [19392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.412030  [19456/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.401228  [19520/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.497871  [19584/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.410297  [19648/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.402643  [19712/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.349817  [19776/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.385991  [19840/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.420317  [19904/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.393243  [19968/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.326143  [20032/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.483837  [20096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.324692  [20160/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.396274  [20224/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.396496  [20288/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.401267  [20352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.354162  [20416/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.392159  [20480/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.317265  [20544/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.409493  [20608/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.381936  [20672/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.429698  [20736/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.356474  [20800/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.486263  [20864/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.375589  [20928/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.340899  [20992/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.458634  [21056/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.522339  [21120/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.398183  [21184/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.584401  [21248/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.384731  [21312/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.366484  [11690/21347]\n",
      "training accuracy: 82.85714387893677%\n",
      "validation accuracy: 93.75%\n",
      "---------------\n",
      "Epoch 6\n",
      "---------------\n",
      "loss: 0.438920  [   64/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.387439  [  128/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.333493  [  192/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.398737  [  256/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.387365  [  320/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.467904  [  384/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.309959  [  448/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.460314  [  512/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.496687  [  576/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.437100  [  640/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.414839  [  704/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.420270  [  768/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.436828  [  832/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.436968  [  896/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.386450  [  960/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.428077  [ 1024/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.462183  [ 1088/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.355982  [ 1152/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.443034  [ 1216/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.349094  [ 1280/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.413862  [ 1344/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.346079  [ 1408/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.509965  [ 1472/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.556723  [ 1536/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.431303  [ 1600/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.476340  [ 1664/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.403701  [ 1728/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.451446  [ 1792/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.370730  [ 1856/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.396435  [ 1920/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.394192  [ 1984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.475316  [ 2048/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.312076  [ 2112/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.568291  [ 2176/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.346655  [ 2240/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.319965  [ 2304/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.410088  [ 2368/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.376440  [ 2432/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.495184  [ 2496/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.401695  [ 2560/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.310819  [ 2624/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.426626  [ 2688/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.451123  [ 2752/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.287986  [ 2816/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.429533  [ 2880/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.415566  [ 2944/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.499138  [ 3008/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.507945  [ 3072/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.362641  [ 3136/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.286181  [ 3200/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.327544  [ 3264/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.533040  [ 3328/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.449741  [ 3392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.430287  [ 3456/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.370045  [ 3520/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.316941  [ 3584/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.401025  [ 3648/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.310536  [ 3712/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.409527  [ 3776/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.435524  [ 3840/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.453721  [ 3904/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.336115  [ 3968/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.388424  [ 4032/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.450062  [ 4096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.333374  [ 4160/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.482157  [ 4224/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.424205  [ 4288/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.463166  [ 4352/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.504670  [ 4416/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.308976  [ 4480/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.302455  [ 4544/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.343455  [ 4608/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.391146  [ 4672/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.432354  [ 4736/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.473241  [ 4800/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.482292  [ 4864/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.535134  [ 4928/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.501166  [ 4992/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.526441  [ 5056/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.421374  [ 5120/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.377012  [ 5184/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.531419  [ 5248/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.351027  [ 5312/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.422526  [ 5376/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.396461  [ 5440/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.435142  [ 5504/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.416007  [ 5568/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.399806  [ 5632/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.495956  [ 5696/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.406083  [ 5760/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.367186  [ 5824/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.461288  [ 5888/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.380343  [ 5952/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.341433  [ 6016/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.428632  [ 6080/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.397344  [ 6144/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.388572  [ 6208/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.423741  [ 6272/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.279257  [ 6336/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.338472  [ 6400/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.399391  [ 6464/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.375348  [ 6528/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.407409  [ 6592/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.369640  [ 6656/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.453736  [ 6720/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.442147  [ 6784/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.369267  [ 6848/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.514643  [ 6912/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.446802  [ 6976/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.351495  [ 7040/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.451510  [ 7104/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.423241  [ 7168/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.473973  [ 7232/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.301717  [ 7296/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.483870  [ 7360/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.586125  [ 7424/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.384401  [ 7488/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.378567  [ 7552/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.488444  [ 7616/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.390358  [ 7680/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.399254  [ 7744/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.501803  [ 7808/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.430054  [ 7872/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.365694  [ 7936/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.515607  [ 8000/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.336442  [ 8064/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.535226  [ 8128/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.367175  [ 8192/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.275794  [ 8256/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.369931  [ 8320/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.445793  [ 8384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.381553  [ 8448/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.484296  [ 8512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.397941  [ 8576/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.344887  [ 8640/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.304136  [ 8704/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.403332  [ 8768/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.438640  [ 8832/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.465468  [ 8896/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.577565  [ 8960/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.621823  [ 9024/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.452842  [ 9088/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.425199  [ 9152/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.422475  [ 9216/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.452799  [ 9280/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.475696  [ 9344/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.365620  [ 9408/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.435645  [ 9472/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.425355  [ 9536/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.442051  [ 9600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.514414  [ 9664/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.347357  [ 9728/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.322310  [ 9792/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.333522  [ 9856/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.416496  [ 9920/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.423786  [ 9984/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.432322  [10048/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.461028  [10112/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.567283  [10176/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.476514  [10240/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.344987  [10304/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.413071  [10368/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.473186  [10432/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.459363  [10496/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.437054  [10560/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.349093  [10624/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.353763  [10688/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.438707  [10752/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.404037  [10816/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.428465  [10880/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.343676  [10944/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.366029  [11008/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.244973  [11072/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.463926  [11136/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.319955  [11200/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.375305  [11264/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.359432  [11328/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.445383  [11392/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.315856  [11456/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.457335  [11520/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.504452  [11584/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.392370  [11648/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.425887  [11712/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.525119  [11776/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.377499  [11840/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.456549  [11904/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.331993  [11968/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.491244  [12032/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.362105  [12096/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.363901  [12160/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.469660  [12224/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.385595  [12288/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.447090  [12352/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.390470  [12416/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.399212  [12480/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.368972  [12544/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.391365  [12608/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.388134  [12672/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.485443  [12736/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.316154  [12800/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.394418  [12864/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.399851  [12928/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.325936  [12992/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.408145  [13056/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.418297  [13120/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.302159  [13184/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.319057  [13248/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.514241  [13312/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.409094  [13376/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.361469  [13440/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.511747  [13504/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.426769  [13568/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.300619  [13632/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.387201  [13696/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.417943  [13760/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.465776  [13824/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.389276  [13888/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.428019  [13952/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.306180  [14016/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.432433  [14080/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.400941  [14144/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.437782  [14208/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.461924  [14272/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.439605  [14336/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.412723  [14400/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.547110  [14464/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.433890  [14528/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.478769  [14592/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.463241  [14656/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.350228  [14720/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.269912  [14784/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.331020  [14848/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.340667  [14912/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.395711  [14976/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.494761  [15040/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.366477  [15104/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.328921  [15168/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.318305  [15232/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.350222  [15296/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.427182  [15360/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.395070  [15424/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.476752  [15488/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.410685  [15552/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.354311  [15616/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.416166  [15680/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.346679  [15744/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.349985  [15808/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.403704  [15872/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.455920  [15936/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.296714  [16000/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.401387  [16064/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.564618  [16128/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.464358  [16192/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.410103  [16256/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.504703  [16320/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.330482  [16384/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.437584  [16448/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.323535  [16512/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.338085  [16576/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.379867  [16640/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.348634  [16704/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.382547  [16768/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.348154  [16832/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.363831  [16896/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.308993  [16960/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.435755  [17024/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.336636  [17088/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.394319  [17152/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.285395  [17216/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.367530  [17280/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.428124  [17344/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.364544  [17408/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.464844  [17472/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.472501  [17536/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.529536  [17600/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.400316  [17664/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.384844  [17728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.478381  [17792/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.479009  [17856/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.385750  [17920/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.405771  [17984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.398595  [18048/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.387594  [18112/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.264623  [18176/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.357377  [18240/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.434850  [18304/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.277108  [18368/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.463159  [18432/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.314090  [18496/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.502631  [18560/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.579082  [18624/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.455058  [18688/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.515551  [18752/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.416718  [18816/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.360495  [18880/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.421248  [18944/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.439268  [19008/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.356612  [19072/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.375515  [19136/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.363133  [19200/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.345701  [19264/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.384637  [19328/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.343440  [19392/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.290583  [19456/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.384509  [19520/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.365440  [19584/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.252718  [19648/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.420306  [19712/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.492018  [19776/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.504098  [19840/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.479115  [19904/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.432078  [19968/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.463273  [20032/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.376447  [20096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.428044  [20160/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.369634  [20224/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.387024  [20288/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.474382  [20352/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.231664  [20416/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.354214  [20480/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.488092  [20544/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.409136  [20608/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.434999  [20672/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.376417  [20736/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.504176  [20800/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.399739  [20864/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.363867  [20928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.566318  [20992/21347]\n",
      "training accuracy: 65.625%\n",
      "loss: 0.369042  [21056/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.416453  [21120/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.361400  [21184/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.479353  [21248/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.401579  [21312/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.286035  [11690/21347]\n",
      "training accuracy: 85.71428656578064%\n",
      "validation accuracy: 95.3125%\n",
      "---------------\n",
      "Epoch 7\n",
      "---------------\n",
      "loss: 0.375034  [   64/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.402271  [  128/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.431348  [  192/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.359983  [  256/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.301288  [  320/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.432974  [  384/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.318498  [  448/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.463801  [  512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.540691  [  576/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.416343  [  640/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.445849  [  704/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.503408  [  768/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.423124  [  832/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.444252  [  896/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.442087  [  960/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.424564  [ 1024/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.454636  [ 1088/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.529293  [ 1152/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.357396  [ 1216/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.337362  [ 1280/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.523605  [ 1344/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.278614  [ 1408/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.366220  [ 1472/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.359527  [ 1536/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.356467  [ 1600/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.322934  [ 1664/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.268349  [ 1728/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.362354  [ 1792/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.584983  [ 1856/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.365144  [ 1920/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.390784  [ 1984/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.367050  [ 2048/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.423702  [ 2112/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.438305  [ 2176/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.384104  [ 2240/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.389909  [ 2304/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.425898  [ 2368/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.415234  [ 2432/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.460246  [ 2496/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.372672  [ 2560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.379834  [ 2624/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.339762  [ 2688/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.479571  [ 2752/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.335020  [ 2816/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.603987  [ 2880/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.444225  [ 2944/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.504073  [ 3008/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.382389  [ 3072/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.394735  [ 3136/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.351173  [ 3200/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.382124  [ 3264/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.322487  [ 3328/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.388084  [ 3392/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.505975  [ 3456/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.466663  [ 3520/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.399105  [ 3584/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.473899  [ 3648/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.354932  [ 3712/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.350955  [ 3776/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.284461  [ 3840/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.374933  [ 3904/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.222706  [ 3968/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.428803  [ 4032/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.313562  [ 4096/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.258524  [ 4160/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.428613  [ 4224/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.296554  [ 4288/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.348285  [ 4352/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.278908  [ 4416/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.573232  [ 4480/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.505183  [ 4544/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.507213  [ 4608/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.356423  [ 4672/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.407342  [ 4736/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.459892  [ 4800/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.365239  [ 4864/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.374921  [ 4928/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.357544  [ 4992/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.419608  [ 5056/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.538666  [ 5120/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.255521  [ 5184/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.409483  [ 5248/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.339856  [ 5312/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.395241  [ 5376/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.383663  [ 5440/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.328434  [ 5504/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.296952  [ 5568/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.404069  [ 5632/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.373241  [ 5696/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.478323  [ 5760/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.286521  [ 5824/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.355903  [ 5888/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.367017  [ 5952/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.318469  [ 6016/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.414320  [ 6080/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.382593  [ 6144/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.485730  [ 6208/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.382050  [ 6272/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.350555  [ 6336/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.437842  [ 6400/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.347750  [ 6464/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.384711  [ 6528/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.471686  [ 6592/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.449792  [ 6656/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.323275  [ 6720/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.326797  [ 6784/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.364738  [ 6848/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.589788  [ 6912/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.394414  [ 6976/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.414146  [ 7040/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.499564  [ 7104/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.462233  [ 7168/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.356245  [ 7232/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.402330  [ 7296/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.449373  [ 7360/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.466215  [ 7424/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.393356  [ 7488/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.460228  [ 7552/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.312079  [ 7616/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.437710  [ 7680/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.468534  [ 7744/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.514064  [ 7808/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.536319  [ 7872/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.561090  [ 7936/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.418480  [ 8000/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.439697  [ 8064/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.438323  [ 8128/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.377216  [ 8192/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.412629  [ 8256/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.443861  [ 8320/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.455592  [ 8384/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.502963  [ 8448/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.383701  [ 8512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.400170  [ 8576/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.351007  [ 8640/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.455417  [ 8704/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.356003  [ 8768/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.477054  [ 8832/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.364236  [ 8896/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.484211  [ 8960/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.381680  [ 9024/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.359902  [ 9088/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.402299  [ 9152/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.487894  [ 9216/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.385844  [ 9280/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.360867  [ 9344/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.491022  [ 9408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.424774  [ 9472/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.312118  [ 9536/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.418598  [ 9600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.319065  [ 9664/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.404325  [ 9728/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.377421  [ 9792/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.345980  [ 9856/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.464502  [ 9920/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.375962  [ 9984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.394985  [10048/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.402893  [10112/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.372661  [10176/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.384416  [10240/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.436569  [10304/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.377943  [10368/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.410124  [10432/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.445210  [10496/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.550947  [10560/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.365012  [10624/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.396943  [10688/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.362806  [10752/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.403215  [10816/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.299988  [10880/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.430101  [10944/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.392915  [11008/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.410561  [11072/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.417488  [11136/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.448863  [11200/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.464711  [11264/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.307474  [11328/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.507450  [11392/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.316303  [11456/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.297733  [11520/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.493638  [11584/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.288049  [11648/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.535497  [11712/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.556377  [11776/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.418176  [11840/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.482165  [11904/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.384576  [11968/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.302897  [12032/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.385069  [12096/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.359853  [12160/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.545336  [12224/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.425264  [12288/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.348125  [12352/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.405492  [12416/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.422654  [12480/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.356844  [12544/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.531405  [12608/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.370036  [12672/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.423466  [12736/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.390144  [12800/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.409075  [12864/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.444366  [12928/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.318587  [12992/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.389799  [13056/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.435345  [13120/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.370900  [13184/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.272086  [13248/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.238022  [13312/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.274972  [13376/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.351778  [13440/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.349029  [13504/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.333773  [13568/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.384127  [13632/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.383187  [13696/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.502360  [13760/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.352302  [13824/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.485711  [13888/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.496052  [13952/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.485154  [14016/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.421931  [14080/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.441193  [14144/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.360374  [14208/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.411495  [14272/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.388113  [14336/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.466899  [14400/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.357181  [14464/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.483404  [14528/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.363173  [14592/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.365526  [14656/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.354575  [14720/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.379254  [14784/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.407887  [14848/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.407335  [14912/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.476263  [14976/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.531742  [15040/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.518351  [15104/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.426667  [15168/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.457166  [15232/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.351332  [15296/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.429634  [15360/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.532659  [15424/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.343883  [15488/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.436856  [15552/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.450446  [15616/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.452020  [15680/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.369109  [15744/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.269738  [15808/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.363025  [15872/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.375565  [15936/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.296908  [16000/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.487170  [16064/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.279070  [16128/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.427572  [16192/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.368145  [16256/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.401563  [16320/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.481627  [16384/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.443934  [16448/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.382425  [16512/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.329806  [16576/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.462657  [16640/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.462560  [16704/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.477748  [16768/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.460404  [16832/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.434363  [16896/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.320303  [16960/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.398907  [17024/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.506466  [17088/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.460166  [17152/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.370011  [17216/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.435547  [17280/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.393206  [17344/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.424952  [17408/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.458825  [17472/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.305898  [17536/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.397859  [17600/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.439199  [17664/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.393803  [17728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.337609  [17792/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.298065  [17856/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.391404  [17920/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.531328  [17984/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.432145  [18048/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.365872  [18112/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.444667  [18176/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.410286  [18240/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.405558  [18304/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.322290  [18368/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.359840  [18432/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.454206  [18496/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.457240  [18560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.383095  [18624/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.492358  [18688/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.272130  [18752/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.321874  [18816/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.402466  [18880/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.441601  [18944/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.362136  [19008/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.376408  [19072/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.401451  [19136/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.427035  [19200/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.404526  [19264/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.447280  [19328/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.363704  [19392/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.339569  [19456/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.362838  [19520/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.430145  [19584/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.374700  [19648/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.333878  [19712/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.403721  [19776/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.402690  [19840/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.492784  [19904/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.507128  [19968/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.340001  [20032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.338861  [20096/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.362405  [20160/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.346392  [20224/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.423290  [20288/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.478390  [20352/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.495203  [20416/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.432319  [20480/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.499547  [20544/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.367494  [20608/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.453350  [20672/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.418262  [20736/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.405978  [20800/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.273453  [20864/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.227918  [20928/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.469935  [20992/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.524436  [21056/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.396631  [21120/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.357073  [21184/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.381016  [21248/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.623387  [21312/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.458639  [11690/21347]\n",
      "training accuracy: 82.85714387893677%\n",
      "validation accuracy: 92.1875%\n",
      "---------------\n",
      "Epoch 8\n",
      "---------------\n",
      "loss: 0.479368  [   64/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.433930  [  128/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.465279  [  192/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.396511  [  256/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.466504  [  320/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.462824  [  384/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.440857  [  448/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.390591  [  512/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.338818  [  576/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.469582  [  640/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.389400  [  704/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.455333  [  768/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.264065  [  832/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.415814  [  896/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.447356  [  960/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.487298  [ 1024/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.353036  [ 1088/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.270929  [ 1152/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.330138  [ 1216/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.371021  [ 1280/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.387208  [ 1344/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.431234  [ 1408/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.320841  [ 1472/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.403756  [ 1536/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.366706  [ 1600/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.440393  [ 1664/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.488276  [ 1728/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.317846  [ 1792/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.414560  [ 1856/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.417710  [ 1920/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.377871  [ 1984/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.305571  [ 2048/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.448626  [ 2112/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.388940  [ 2176/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.391226  [ 2240/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.505141  [ 2304/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.333541  [ 2368/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.334096  [ 2432/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.387548  [ 2496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.371065  [ 2560/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.433522  [ 2624/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.378653  [ 2688/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.371255  [ 2752/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.413293  [ 2816/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.388009  [ 2880/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.279688  [ 2944/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.534239  [ 3008/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.489442  [ 3072/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.290020  [ 3136/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.454496  [ 3200/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.428746  [ 3264/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.502159  [ 3328/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.397095  [ 3392/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.372858  [ 3456/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.421381  [ 3520/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.508379  [ 3584/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.413787  [ 3648/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.438451  [ 3712/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.382249  [ 3776/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.326895  [ 3840/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.402704  [ 3904/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.336544  [ 3968/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.490475  [ 4032/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.396682  [ 4096/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.429891  [ 4160/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.451370  [ 4224/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.397109  [ 4288/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.389089  [ 4352/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.382010  [ 4416/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.459170  [ 4480/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.413807  [ 4544/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.319401  [ 4608/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.400768  [ 4672/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.541645  [ 4736/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.426208  [ 4800/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.467405  [ 4864/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.423541  [ 4928/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.446797  [ 4992/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.335043  [ 5056/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.410726  [ 5120/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.465604  [ 5184/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.401359  [ 5248/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.373792  [ 5312/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.392234  [ 5376/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.373830  [ 5440/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.441718  [ 5504/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.390124  [ 5568/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.492545  [ 5632/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.329085  [ 5696/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.451210  [ 5760/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.279864  [ 5824/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.320648  [ 5888/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.414798  [ 5952/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.375800  [ 6016/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.463696  [ 6080/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.431054  [ 6144/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.414507  [ 6208/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.541124  [ 6272/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.513294  [ 6336/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.356013  [ 6400/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.405459  [ 6464/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.426491  [ 6528/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.315616  [ 6592/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.407430  [ 6656/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.380322  [ 6720/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.372767  [ 6784/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.405787  [ 6848/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.390232  [ 6912/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.554010  [ 6976/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.491754  [ 7040/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.386965  [ 7104/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.431319  [ 7168/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.377677  [ 7232/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.428905  [ 7296/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.273846  [ 7360/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.482864  [ 7424/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.315297  [ 7488/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.378975  [ 7552/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.395914  [ 7616/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.284140  [ 7680/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.281648  [ 7744/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.391856  [ 7808/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.393896  [ 7872/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.373415  [ 7936/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.328212  [ 8000/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.419456  [ 8064/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.481916  [ 8128/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.357910  [ 8192/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.429872  [ 8256/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.310369  [ 8320/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.375321  [ 8384/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.487874  [ 8448/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.407370  [ 8512/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.309555  [ 8576/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.305764  [ 8640/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.477917  [ 8704/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.373212  [ 8768/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.372392  [ 8832/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.365701  [ 8896/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.350507  [ 8960/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.425859  [ 9024/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.363147  [ 9088/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.379774  [ 9152/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.475163  [ 9216/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.386354  [ 9280/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.416856  [ 9344/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.375142  [ 9408/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.395324  [ 9472/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.435628  [ 9536/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.372466  [ 9600/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.360047  [ 9664/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.416126  [ 9728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.452020  [ 9792/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.393933  [ 9856/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.342859  [ 9920/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.373658  [ 9984/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.355770  [10048/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.383905  [10112/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.457084  [10176/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.572851  [10240/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.520024  [10304/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.467693  [10368/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.372046  [10432/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.441303  [10496/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.506407  [10560/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.425796  [10624/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.326926  [10688/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.325492  [10752/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.449974  [10816/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.466169  [10880/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.398619  [10944/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.425588  [11008/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.390131  [11072/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.455677  [11136/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.276257  [11200/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.356097  [11264/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.433981  [11328/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.374215  [11392/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.342183  [11456/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.231961  [11520/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.304185  [11584/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.399035  [11648/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.337086  [11712/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.423476  [11776/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.503837  [11840/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.446313  [11904/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.388479  [11968/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.387836  [12032/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.394641  [12096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.299846  [12160/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.239352  [12224/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.338394  [12288/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.371395  [12352/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.438969  [12416/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.376038  [12480/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.474734  [12544/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.365231  [12608/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.495748  [12672/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.403486  [12736/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.385653  [12800/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.402062  [12864/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.382800  [12928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.380659  [12992/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.336059  [13056/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.493615  [13120/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.426680  [13184/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.338983  [13248/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.401964  [13312/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.372923  [13376/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.453587  [13440/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.323244  [13504/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.345921  [13568/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.463476  [13632/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.499333  [13696/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.537712  [13760/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.403871  [13824/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.503074  [13888/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.401902  [13952/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.400948  [14016/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.375054  [14080/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.440215  [14144/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.349211  [14208/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.363952  [14272/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.361370  [14336/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.358616  [14400/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.449204  [14464/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.419887  [14528/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.478736  [14592/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.313627  [14656/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.245491  [14720/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.341287  [14784/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.478531  [14848/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.504335  [14912/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.347022  [14976/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.430839  [15040/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.382757  [15104/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.356997  [15168/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.325887  [15232/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.480573  [15296/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.378621  [15360/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.383720  [15424/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.421816  [15488/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.347291  [15552/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.404661  [15616/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.386017  [15680/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.508293  [15744/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.436063  [15808/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.349975  [15872/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.297474  [15936/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.298175  [16000/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.384336  [16064/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.276085  [16128/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.405563  [16192/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.471172  [16256/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.437348  [16320/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.352246  [16384/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.327878  [16448/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.340848  [16512/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.356210  [16576/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.323641  [16640/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.363747  [16704/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.279639  [16768/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.314584  [16832/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.481233  [16896/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.327096  [16960/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.453074  [17024/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.541604  [17088/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.594732  [17152/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.484444  [17216/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.380572  [17280/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.399798  [17344/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.444734  [17408/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.367528  [17472/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.349005  [17536/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.436777  [17600/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.449565  [17664/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.414703  [17728/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.367953  [17792/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.326742  [17856/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.285038  [17920/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.512381  [17984/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.578022  [18048/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.418741  [18112/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.494235  [18176/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.419611  [18240/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.419554  [18304/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.431882  [18368/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.418051  [18432/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.417186  [18496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.461856  [18560/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.402584  [18624/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.318213  [18688/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.372263  [18752/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.323563  [18816/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.413509  [18880/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.257619  [18944/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.404044  [19008/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.503467  [19072/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.325275  [19136/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.412165  [19200/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.397390  [19264/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.399596  [19328/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.479238  [19392/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.326342  [19456/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.337751  [19520/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.401476  [19584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.382088  [19648/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.369530  [19712/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.372475  [19776/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.434060  [19840/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.359768  [19904/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.365341  [19968/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.529625  [20032/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.309851  [20096/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.332740  [20160/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.287073  [20224/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.425686  [20288/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.384837  [20352/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.316227  [20416/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.332071  [20480/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.336302  [20544/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.586998  [20608/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.354756  [20672/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.373723  [20736/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.317587  [20800/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.405404  [20864/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.358604  [20928/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.462685  [20992/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.448639  [21056/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.421482  [21120/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.333750  [21184/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.498066  [21248/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.469438  [21312/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.319209  [11690/21347]\n",
      "training accuracy: 91.42857193946838%\n",
      "validation accuracy: 89.0625%\n",
      "---------------\n",
      "Epoch 9\n",
      "---------------\n",
      "loss: 0.446371  [   64/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.388269  [  128/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.295934  [  192/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.456413  [  256/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.457430  [  320/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.398270  [  384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.307875  [  448/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.334409  [  512/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.453962  [  576/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.256019  [  640/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.336109  [  704/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.416844  [  768/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.399551  [  832/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.344617  [  896/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.365542  [  960/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.407925  [ 1024/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.476277  [ 1088/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.418925  [ 1152/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.432514  [ 1216/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.365065  [ 1280/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.516842  [ 1344/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.385151  [ 1408/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.364488  [ 1472/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.300756  [ 1536/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.422754  [ 1600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.295593  [ 1664/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.347313  [ 1728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.371544  [ 1792/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.440691  [ 1856/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.444389  [ 1920/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.412546  [ 1984/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.419618  [ 2048/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.515476  [ 2112/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.399877  [ 2176/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.283650  [ 2240/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.326234  [ 2304/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.301978  [ 2368/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.402881  [ 2432/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.409646  [ 2496/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.473295  [ 2560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.343200  [ 2624/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.463136  [ 2688/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.586849  [ 2752/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.483347  [ 2816/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.431950  [ 2880/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.270940  [ 2944/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.329109  [ 3008/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.471884  [ 3072/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.342210  [ 3136/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.468997  [ 3200/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.361319  [ 3264/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.446299  [ 3328/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.398170  [ 3392/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.387131  [ 3456/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.395567  [ 3520/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.331271  [ 3584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.465159  [ 3648/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.478762  [ 3712/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.446750  [ 3776/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.340944  [ 3840/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.364695  [ 3904/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.418226  [ 3968/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.353574  [ 4032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.390442  [ 4096/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.393960  [ 4160/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.401741  [ 4224/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.431253  [ 4288/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.424672  [ 4352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.330839  [ 4416/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.363964  [ 4480/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.641559  [ 4544/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.354675  [ 4608/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.556363  [ 4672/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.392473  [ 4736/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.418226  [ 4800/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.385320  [ 4864/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.414133  [ 4928/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.457172  [ 4992/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.518914  [ 5056/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.529778  [ 5120/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.400381  [ 5184/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.596391  [ 5248/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.333710  [ 5312/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.431818  [ 5376/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.403589  [ 5440/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.364966  [ 5504/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.388417  [ 5568/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.493511  [ 5632/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.404110  [ 5696/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.336697  [ 5760/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.383025  [ 5824/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.382415  [ 5888/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.436733  [ 5952/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.413500  [ 6016/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.397885  [ 6080/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.421335  [ 6144/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.356836  [ 6208/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.483884  [ 6272/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.353888  [ 6336/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.368111  [ 6400/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.420523  [ 6464/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.381815  [ 6528/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.398850  [ 6592/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.422890  [ 6656/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.288561  [ 6720/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.278508  [ 6784/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.446287  [ 6848/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.278103  [ 6912/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.305906  [ 6976/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.347947  [ 7040/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.359769  [ 7104/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.392764  [ 7168/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.345328  [ 7232/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.483747  [ 7296/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.495066  [ 7360/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.532737  [ 7424/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.306150  [ 7488/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.436173  [ 7552/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.379301  [ 7616/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.361391  [ 7680/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.379069  [ 7744/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.424488  [ 7808/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.423850  [ 7872/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.524364  [ 7936/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.398295  [ 8000/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.353605  [ 8064/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.366571  [ 8128/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.548331  [ 8192/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.352797  [ 8256/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.460273  [ 8320/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.500619  [ 8384/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.413833  [ 8448/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.380647  [ 8512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.415654  [ 8576/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.437439  [ 8640/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.380423  [ 8704/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.459547  [ 8768/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.316803  [ 8832/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.453105  [ 8896/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.340540  [ 8960/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.429365  [ 9024/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.527895  [ 9088/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.324656  [ 9152/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.354049  [ 9216/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.412349  [ 9280/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.387305  [ 9344/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.315830  [ 9408/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.466908  [ 9472/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.324633  [ 9536/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.444162  [ 9600/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.314059  [ 9664/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.391644  [ 9728/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.370586  [ 9792/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.495169  [ 9856/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.332814  [ 9920/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.325461  [ 9984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.406539  [10048/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.302373  [10112/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.442893  [10176/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.283830  [10240/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.334642  [10304/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.444184  [10368/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.253703  [10432/21347]\n",
      "training accuracy: 95.3125%\n",
      "loss: 0.414079  [10496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.456533  [10560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.325792  [10624/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.478718  [10688/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.360843  [10752/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.381771  [10816/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.522608  [10880/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.443536  [10944/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.505180  [11008/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.449047  [11072/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.417388  [11136/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.377722  [11200/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.430937  [11264/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.454043  [11328/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.421463  [11392/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.432091  [11456/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.389829  [11520/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.359575  [11584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.374152  [11648/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.333124  [11712/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.470086  [11776/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.391306  [11840/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.567112  [11904/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.479659  [11968/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.440790  [12032/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.366797  [12096/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.331447  [12160/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.478130  [12224/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.332658  [12288/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.378820  [12352/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.300218  [12416/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.345564  [12480/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.380790  [12544/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.431502  [12608/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.211778  [12672/21347]\n",
      "training accuracy: 95.3125%\n",
      "loss: 0.405701  [12736/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.324433  [12800/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.406146  [12864/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.542907  [12928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.486901  [12992/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.326267  [13056/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.350671  [13120/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.433385  [13184/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.339439  [13248/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.379866  [13312/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.538566  [13376/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.449937  [13440/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.417412  [13504/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.303557  [13568/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.288990  [13632/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.425066  [13696/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.400634  [13760/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.460655  [13824/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.314141  [13888/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.381783  [13952/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.323158  [14016/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.371954  [14080/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.387447  [14144/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.351365  [14208/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.453787  [14272/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.402468  [14336/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.445255  [14400/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.398842  [14464/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.324581  [14528/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.399383  [14592/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.398185  [14656/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.351022  [14720/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.276819  [14784/21347]\n",
      "training accuracy: 95.3125%\n",
      "loss: 0.388403  [14848/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.448850  [14912/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.334207  [14976/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.350252  [15040/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.474228  [15104/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.431508  [15168/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.364105  [15232/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.369616  [15296/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.470978  [15360/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.360372  [15424/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.382151  [15488/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.278734  [15552/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.469926  [15616/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.295283  [15680/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.213024  [15744/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.385323  [15808/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.416592  [15872/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.350599  [15936/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.329914  [16000/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.378326  [16064/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.354416  [16128/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.434892  [16192/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.478307  [16256/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.411278  [16320/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.525667  [16384/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.314825  [16448/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.364412  [16512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.286567  [16576/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.419224  [16640/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.484374  [16704/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.270553  [16768/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.435795  [16832/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.361036  [16896/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.343995  [16960/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.548746  [17024/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.526062  [17088/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.324924  [17152/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.314591  [17216/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.479550  [17280/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.390956  [17344/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.371329  [17408/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.443821  [17472/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.330162  [17536/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.459598  [17600/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.450173  [17664/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.368780  [17728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.278431  [17792/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.504370  [17856/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.363369  [17920/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.308217  [17984/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.386985  [18048/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.334114  [18112/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.363843  [18176/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.320342  [18240/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.425867  [18304/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.457707  [18368/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.490443  [18432/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.381623  [18496/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.467137  [18560/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.373851  [18624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.436247  [18688/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.487463  [18752/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.479614  [18816/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.503604  [18880/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.396216  [18944/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.384628  [19008/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.436100  [19072/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.511678  [19136/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.319115  [19200/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.428365  [19264/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.352229  [19328/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.366582  [19392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.374370  [19456/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.453670  [19520/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.423989  [19584/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.466031  [19648/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.361465  [19712/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.373264  [19776/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.310120  [19840/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.532296  [19904/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.367633  [19968/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.426578  [20032/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.290582  [20096/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.369962  [20160/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.338751  [20224/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.412948  [20288/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.270044  [20352/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.303800  [20416/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.352608  [20480/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.362929  [20544/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.313414  [20608/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.333306  [20672/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.536224  [20736/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.501827  [20800/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.440775  [20864/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.277061  [20928/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.411085  [20992/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.286052  [21056/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.476737  [21120/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.309015  [21184/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.484124  [21248/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.388910  [21312/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.364189  [11690/21347]\n",
      "training accuracy: 82.85714387893677%\n",
      "validation accuracy: 100.0%\n",
      "---------------\n",
      "Epoch 10\n",
      "---------------\n",
      "loss: 0.398317  [   64/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.291221  [  128/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.476728  [  192/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.349527  [  256/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.368969  [  320/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.583295  [  384/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.444435  [  448/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.610670  [  512/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.430285  [  576/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.448654  [  640/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.479350  [  704/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.498798  [  768/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.433039  [  832/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.399031  [  896/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.345254  [  960/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.379079  [ 1024/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.325979  [ 1088/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.440026  [ 1152/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.408411  [ 1216/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.407050  [ 1280/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.296544  [ 1344/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.348543  [ 1408/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.462836  [ 1472/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.487646  [ 1536/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.290855  [ 1600/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.429676  [ 1664/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.395550  [ 1728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.337942  [ 1792/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.341013  [ 1856/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.405022  [ 1920/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.296870  [ 1984/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.337681  [ 2048/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.544120  [ 2112/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.372180  [ 2176/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.317885  [ 2240/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.433823  [ 2304/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.268347  [ 2368/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.335002  [ 2432/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.335243  [ 2496/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.341573  [ 2560/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.464989  [ 2624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.367869  [ 2688/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.387677  [ 2752/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.417751  [ 2816/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.383631  [ 2880/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.378808  [ 2944/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.247602  [ 3008/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.406436  [ 3072/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.388672  [ 3136/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.388406  [ 3200/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.451812  [ 3264/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.264891  [ 3328/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.490649  [ 3392/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.436317  [ 3456/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.511942  [ 3520/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.371794  [ 3584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.358990  [ 3648/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.408422  [ 3712/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.302512  [ 3776/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.395075  [ 3840/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.493980  [ 3904/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.383049  [ 3968/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.426832  [ 4032/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.268399  [ 4096/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.449673  [ 4160/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.319961  [ 4224/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.446611  [ 4288/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.359343  [ 4352/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.475593  [ 4416/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.463068  [ 4480/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.441111  [ 4544/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.454226  [ 4608/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.545679  [ 4672/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.352308  [ 4736/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.361951  [ 4800/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.444350  [ 4864/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.327114  [ 4928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.413409  [ 4992/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.372157  [ 5056/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.395117  [ 5120/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.412327  [ 5184/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.341805  [ 5248/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.319518  [ 5312/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.333758  [ 5376/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.370111  [ 5440/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.389753  [ 5504/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.478081  [ 5568/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.363653  [ 5632/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.449089  [ 5696/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.420657  [ 5760/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.331709  [ 5824/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.297223  [ 5888/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.438946  [ 5952/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.373423  [ 6016/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.329418  [ 6080/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.322124  [ 6144/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.304915  [ 6208/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.519649  [ 6272/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.629626  [ 6336/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.372252  [ 6400/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.506188  [ 6464/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.431820  [ 6528/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.433135  [ 6592/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.351959  [ 6656/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.447327  [ 6720/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.411026  [ 6784/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.421124  [ 6848/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.368347  [ 6912/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.378755  [ 6976/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.440210  [ 7040/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.243618  [ 7104/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.340342  [ 7168/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.366091  [ 7232/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.329690  [ 7296/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.392820  [ 7360/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.300296  [ 7424/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.513500  [ 7488/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.556076  [ 7552/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.493659  [ 7616/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.400485  [ 7680/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.378126  [ 7744/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.454288  [ 7808/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.412051  [ 7872/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.420744  [ 7936/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.378226  [ 8000/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.295676  [ 8064/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.451123  [ 8128/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.394570  [ 8192/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.473693  [ 8256/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.430444  [ 8320/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.468168  [ 8384/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.423137  [ 8448/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.377751  [ 8512/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.435280  [ 8576/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.369379  [ 8640/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.351624  [ 8704/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.408730  [ 8768/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.417040  [ 8832/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.444093  [ 8896/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.326301  [ 8960/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.366908  [ 9024/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.419716  [ 9088/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.400072  [ 9152/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.468149  [ 9216/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.336487  [ 9280/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.498760  [ 9344/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.399666  [ 9408/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.278374  [ 9472/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.280128  [ 9536/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.445912  [ 9600/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.332079  [ 9664/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.429844  [ 9728/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.340030  [ 9792/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.493552  [ 9856/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.573128  [ 9920/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.325080  [ 9984/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.345748  [10048/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.421773  [10112/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.369431  [10176/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.386006  [10240/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.345563  [10304/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.304107  [10368/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.322326  [10432/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.574702  [10496/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.486447  [10560/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.499632  [10624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.407000  [10688/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.399922  [10752/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.339960  [10816/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.379319  [10880/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.380915  [10944/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.256726  [11008/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.411434  [11072/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.348771  [11136/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.416363  [11200/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.344141  [11264/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.294264  [11328/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.457136  [11392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.405469  [11456/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.467628  [11520/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.375640  [11584/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.257969  [11648/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.390234  [11712/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.325889  [11776/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.353838  [11840/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.477986  [11904/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.306600  [11968/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.477161  [12032/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.413170  [12096/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.472434  [12160/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.357533  [12224/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.366788  [12288/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.393259  [12352/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.378691  [12416/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.284857  [12480/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.529498  [12544/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.514264  [12608/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.424206  [12672/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.427225  [12736/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.384089  [12800/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.312316  [12864/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.316707  [12928/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.455033  [12992/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.254881  [13056/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.434799  [13120/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.395147  [13184/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.301628  [13248/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.269881  [13312/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.370156  [13376/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.466187  [13440/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.352615  [13504/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.474884  [13568/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.391696  [13632/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.251328  [13696/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.495537  [13760/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.268771  [13824/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.390193  [13888/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.369275  [13952/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.435616  [14016/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.396264  [14080/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.513022  [14144/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.560212  [14208/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.364776  [14272/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.315502  [14336/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.424960  [14400/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.542549  [14464/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.370851  [14528/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.493977  [14592/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.346757  [14656/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.419225  [14720/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.423481  [14784/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.306896  [14848/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.508949  [14912/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.371509  [14976/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.429552  [15040/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.518663  [15104/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.573185  [15168/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.435663  [15232/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.402351  [15296/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.399344  [15360/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.337611  [15424/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.292221  [15488/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.252633  [15552/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.586636  [15616/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.251595  [15680/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.423350  [15744/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.467405  [15808/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.385967  [15872/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.357741  [15936/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.448296  [16000/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.318230  [16064/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.450570  [16128/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.301370  [16192/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.447554  [16256/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.415338  [16320/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.394352  [16384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.417788  [16448/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.328625  [16512/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.397439  [16576/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.321427  [16640/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.456161  [16704/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.399676  [16768/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.294096  [16832/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.395436  [16896/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.301454  [16960/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.427462  [17024/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.381670  [17088/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.515389  [17152/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.405704  [17216/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.389342  [17280/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.482641  [17344/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.374355  [17408/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.308778  [17472/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.470099  [17536/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.456827  [17600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.241662  [17664/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.340451  [17728/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.422667  [17792/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.350291  [17856/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.421058  [17920/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.413722  [17984/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.284328  [18048/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.419297  [18112/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.392321  [18176/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.337669  [18240/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.524887  [18304/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.364801  [18368/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.351395  [18432/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.419767  [18496/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.426863  [18560/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.434002  [18624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.504589  [18688/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.371617  [18752/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.352657  [18816/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.413233  [18880/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.468559  [18944/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.331419  [19008/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.339161  [19072/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.369873  [19136/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.322143  [19200/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.310614  [19264/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.340691  [19328/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.495574  [19392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.349955  [19456/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.420401  [19520/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.416384  [19584/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.453780  [19648/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.449163  [19712/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.418655  [19776/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.364059  [19840/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.404407  [19904/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.449895  [19968/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.532184  [20032/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.422620  [20096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.408887  [20160/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.437858  [20224/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.336308  [20288/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.432290  [20352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.427403  [20416/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.349461  [20480/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.321326  [20544/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.364305  [20608/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.412572  [20672/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.361335  [20736/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.306886  [20800/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.463268  [20864/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.367856  [20928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.425941  [20992/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.476187  [21056/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.328112  [21120/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.371574  [21184/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.367473  [21248/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.284270  [21312/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.234880  [11690/21347]\n",
      "training accuracy: 91.42857193946838%\n",
      "validation accuracy: 98.4375%\n",
      "---------------\n",
      "Epoch 11\n",
      "---------------\n",
      "loss: 0.432564  [   64/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.452943  [  128/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.449890  [  192/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.371974  [  256/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.411167  [  320/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.307555  [  384/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.359976  [  448/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.412894  [  512/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.401208  [  576/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.324542  [  640/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.300991  [  704/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.356905  [  768/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.527200  [  832/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.345569  [  896/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.330353  [  960/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.467818  [ 1024/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.489245  [ 1088/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.382986  [ 1152/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.380184  [ 1216/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.518729  [ 1280/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.481335  [ 1344/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.414797  [ 1408/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.434136  [ 1472/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.410079  [ 1536/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.432251  [ 1600/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.371668  [ 1664/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.458388  [ 1728/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.462173  [ 1792/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.448145  [ 1856/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.454158  [ 1920/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.356492  [ 1984/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.424867  [ 2048/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.344944  [ 2112/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.446104  [ 2176/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.289970  [ 2240/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.386963  [ 2304/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.343361  [ 2368/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.259952  [ 2432/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.317517  [ 2496/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.271603  [ 2560/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.413190  [ 2624/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.419736  [ 2688/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.365896  [ 2752/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.333678  [ 2816/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.448464  [ 2880/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.503094  [ 2944/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.357954  [ 3008/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.274512  [ 3072/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.392056  [ 3136/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.579299  [ 3200/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.420442  [ 3264/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.377968  [ 3328/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.445077  [ 3392/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.319378  [ 3456/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.327263  [ 3520/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.351317  [ 3584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.430814  [ 3648/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.436098  [ 3712/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.357730  [ 3776/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.363258  [ 3840/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.343341  [ 3904/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.520347  [ 3968/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.422384  [ 4032/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.369220  [ 4096/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.359389  [ 4160/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.429248  [ 4224/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.361488  [ 4288/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.368990  [ 4352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.390891  [ 4416/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.298563  [ 4480/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.408745  [ 4544/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.453272  [ 4608/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.397300  [ 4672/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.330470  [ 4736/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.431212  [ 4800/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.377400  [ 4864/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.260349  [ 4928/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.395932  [ 4992/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.482765  [ 5056/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.529451  [ 5120/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.343458  [ 5184/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.409228  [ 5248/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.471447  [ 5312/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.377532  [ 5376/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.444637  [ 5440/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.404391  [ 5504/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.324141  [ 5568/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.439542  [ 5632/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.483808  [ 5696/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.345155  [ 5760/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.441068  [ 5824/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.286994  [ 5888/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.395864  [ 5952/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.418254  [ 6016/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.338445  [ 6080/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.395438  [ 6144/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.333934  [ 6208/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.335305  [ 6272/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.396147  [ 6336/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.336239  [ 6400/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.378256  [ 6464/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.379901  [ 6528/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.452617  [ 6592/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.353192  [ 6656/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.383770  [ 6720/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.373233  [ 6784/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.425549  [ 6848/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.260249  [ 6912/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.491904  [ 6976/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.421925  [ 7040/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.443531  [ 7104/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.411505  [ 7168/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.474401  [ 7232/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.311794  [ 7296/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.336533  [ 7360/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.391046  [ 7424/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.338899  [ 7488/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.318054  [ 7552/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.364287  [ 7616/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.371436  [ 7680/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.437799  [ 7744/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.516491  [ 7808/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.383118  [ 7872/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.361897  [ 7936/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.427542  [ 8000/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.413959  [ 8064/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.444064  [ 8128/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.378415  [ 8192/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.372709  [ 8256/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.370169  [ 8320/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.462383  [ 8384/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.255748  [ 8448/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.406159  [ 8512/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.352988  [ 8576/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.311616  [ 8640/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.329061  [ 8704/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.324655  [ 8768/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.452255  [ 8832/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.452217  [ 8896/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.383602  [ 8960/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.464761  [ 9024/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.467756  [ 9088/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.497947  [ 9152/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.358695  [ 9216/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.282134  [ 9280/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.256158  [ 9344/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.315672  [ 9408/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.360518  [ 9472/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.435906  [ 9536/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.456373  [ 9600/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.527722  [ 9664/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.400146  [ 9728/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.510077  [ 9792/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.395913  [ 9856/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.395415  [ 9920/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.431297  [ 9984/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.236352  [10048/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.236152  [10112/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.358018  [10176/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.362627  [10240/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.418442  [10304/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.369958  [10368/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.403205  [10432/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.492389  [10496/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.372354  [10560/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.445452  [10624/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.387964  [10688/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.391604  [10752/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.308484  [10816/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.433976  [10880/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.382932  [10944/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.476928  [11008/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.376646  [11072/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.253952  [11136/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.300763  [11200/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.341315  [11264/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.416928  [11328/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.497628  [11392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.448662  [11456/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.330218  [11520/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.273365  [11584/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.400588  [11648/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.451363  [11712/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.328011  [11776/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.337681  [11840/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.371729  [11904/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.397615  [11968/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.332288  [12032/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.393576  [12096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.259468  [12160/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.373335  [12224/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.465452  [12288/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.575218  [12352/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.440940  [12416/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.388617  [12480/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.438804  [12544/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.400087  [12608/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.379826  [12672/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.408365  [12736/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.446633  [12800/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.372426  [12864/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.413095  [12928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.388818  [12992/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.185659  [13056/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.413138  [13120/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.403858  [13184/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.484486  [13248/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.398704  [13312/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.536524  [13376/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.358579  [13440/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.454062  [13504/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.349478  [13568/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.491552  [13632/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.452259  [13696/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.324201  [13760/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.412669  [13824/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.366297  [13888/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.348894  [13952/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.328398  [14016/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.510916  [14080/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.412774  [14144/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.296397  [14208/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.533163  [14272/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.458307  [14336/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.346580  [14400/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.333488  [14464/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.335570  [14528/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.423060  [14592/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.373404  [14656/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.443347  [14720/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.408035  [14784/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.362286  [14848/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.334040  [14912/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.334490  [14976/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.429707  [15040/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.331302  [15104/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.377200  [15168/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.330844  [15232/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.379052  [15296/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.475356  [15360/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.225761  [15424/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.420462  [15488/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.483864  [15552/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.355473  [15616/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.339293  [15680/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.328898  [15744/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.395032  [15808/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.517981  [15872/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.418140  [15936/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.344427  [16000/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.532664  [16064/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.389067  [16128/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.438743  [16192/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.407053  [16256/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.362621  [16320/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.432653  [16384/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.479618  [16448/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.354773  [16512/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.384998  [16576/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.284649  [16640/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.358944  [16704/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.333191  [16768/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.429662  [16832/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.428711  [16896/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.360553  [16960/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.277744  [17024/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.319067  [17088/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.379995  [17152/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.566921  [17216/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.315298  [17280/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.395028  [17344/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.498496  [17408/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.335124  [17472/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.325258  [17536/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.316486  [17600/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.303035  [17664/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.347435  [17728/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.355662  [17792/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.365372  [17856/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.498819  [17920/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.319813  [17984/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.318020  [18048/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.360676  [18112/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.399721  [18176/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.500719  [18240/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.404433  [18304/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.281362  [18368/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.539118  [18432/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.472537  [18496/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.277863  [18560/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.390939  [18624/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.415311  [18688/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.436178  [18752/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.396081  [18816/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.456385  [18880/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.421958  [18944/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.398480  [19008/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.412558  [19072/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.311428  [19136/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.416368  [19200/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.458093  [19264/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.465723  [19328/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.375980  [19392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.484929  [19456/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.383613  [19520/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.365040  [19584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.392916  [19648/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.323588  [19712/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.399572  [19776/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.506041  [19840/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.436558  [19904/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.442896  [19968/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.390241  [20032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.444024  [20096/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.374303  [20160/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.339553  [20224/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.263410  [20288/21347]\n",
      "training accuracy: 96.875%\n",
      "loss: 0.349068  [20352/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.405784  [20416/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.537568  [20480/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.473679  [20544/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.285873  [20608/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.454238  [20672/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.394437  [20736/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.374000  [20800/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.451826  [20864/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.399552  [20928/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.402867  [20992/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.331847  [21056/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.517769  [21120/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.433279  [21184/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.430225  [21248/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.356300  [21312/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.320753  [11690/21347]\n",
      "training accuracy: 80.0000011920929%\n",
      "validation accuracy: 96.875%\n",
      "---------------\n",
      "Epoch 12\n",
      "---------------\n",
      "loss: 0.442037  [   64/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.509932  [  128/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.542401  [  192/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.365623  [  256/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.375532  [  320/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.392444  [  384/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.391388  [  448/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.314546  [  512/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.311966  [  576/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.340680  [  640/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.382044  [  704/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.416524  [  768/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.391674  [  832/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.396838  [  896/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.419824  [  960/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.417194  [ 1024/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.433624  [ 1088/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.505347  [ 1152/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.481972  [ 1216/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.480164  [ 1280/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.415156  [ 1344/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.550329  [ 1408/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.356902  [ 1472/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.437816  [ 1536/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.310836  [ 1600/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.419097  [ 1664/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.467007  [ 1728/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.495034  [ 1792/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.425294  [ 1856/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.442876  [ 1920/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.309127  [ 1984/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.452358  [ 2048/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.500030  [ 2112/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.368647  [ 2176/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.411122  [ 2240/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.217451  [ 2304/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.419903  [ 2368/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.388814  [ 2432/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.358364  [ 2496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.401190  [ 2560/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.311569  [ 2624/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.399632  [ 2688/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.319405  [ 2752/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.310213  [ 2816/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.462707  [ 2880/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.418422  [ 2944/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.268628  [ 3008/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.292985  [ 3072/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.401304  [ 3136/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.324245  [ 3200/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.303612  [ 3264/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.274399  [ 3328/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.490652  [ 3392/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.482295  [ 3456/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.327501  [ 3520/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.338875  [ 3584/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.476485  [ 3648/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.470645  [ 3712/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.345656  [ 3776/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.473822  [ 3840/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.312998  [ 3904/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.437211  [ 3968/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.383016  [ 4032/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.338916  [ 4096/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.460309  [ 4160/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.342534  [ 4224/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.357696  [ 4288/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.353205  [ 4352/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.389029  [ 4416/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.369672  [ 4480/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.340879  [ 4544/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.371047  [ 4608/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.320442  [ 4672/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.439023  [ 4736/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.552505  [ 4800/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.340829  [ 4864/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.309925  [ 4928/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.306176  [ 4992/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.367248  [ 5056/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.353172  [ 5120/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.397662  [ 5184/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.430615  [ 5248/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.384715  [ 5312/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.387323  [ 5376/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.346942  [ 5440/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.550277  [ 5504/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.368108  [ 5568/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.488243  [ 5632/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.529002  [ 5696/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.489922  [ 5760/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.327637  [ 5824/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.323641  [ 5888/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.337614  [ 5952/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.401560  [ 6016/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.340212  [ 6080/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.397878  [ 6144/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.353432  [ 6208/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.395073  [ 6272/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.356544  [ 6336/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.408383  [ 6400/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.426272  [ 6464/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.461661  [ 6528/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.424741  [ 6592/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.417862  [ 6656/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.375836  [ 6720/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.277584  [ 6784/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.336909  [ 6848/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.364429  [ 6912/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.437569  [ 6976/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.316752  [ 7040/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.364763  [ 7104/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.420105  [ 7168/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.464974  [ 7232/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.373260  [ 7296/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.243116  [ 7360/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.532001  [ 7424/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.350070  [ 7488/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.392690  [ 7552/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.419964  [ 7616/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.416938  [ 7680/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.257404  [ 7744/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.407120  [ 7808/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.508451  [ 7872/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.337251  [ 7936/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.448097  [ 8000/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.367559  [ 8064/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.347239  [ 8128/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.386445  [ 8192/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.612259  [ 8256/21347]\n",
      "training accuracy: 65.625%\n",
      "loss: 0.498334  [ 8320/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.336494  [ 8384/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.333646  [ 8448/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.347355  [ 8512/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.407765  [ 8576/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.455135  [ 8640/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.363333  [ 8704/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.405430  [ 8768/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.388173  [ 8832/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.436761  [ 8896/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.399168  [ 8960/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.359866  [ 9024/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.497190  [ 9088/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.442755  [ 9152/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.306370  [ 9216/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.275230  [ 9280/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.327476  [ 9344/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.474584  [ 9408/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.324860  [ 9472/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.435717  [ 9536/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.338265  [ 9600/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.337470  [ 9664/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.379487  [ 9728/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.377649  [ 9792/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.431665  [ 9856/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.374385  [ 9920/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.293668  [ 9984/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.454294  [10048/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.307560  [10112/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.455171  [10176/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.399999  [10240/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.414397  [10304/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.281874  [10368/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.408321  [10432/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.428665  [10496/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.267191  [10560/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.506411  [10624/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.432632  [10688/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.365327  [10752/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.422366  [10816/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.391239  [10880/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.403716  [10944/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.438063  [11008/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.340962  [11072/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.328783  [11136/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.333887  [11200/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.456150  [11264/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.315791  [11328/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.463302  [11392/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.294030  [11456/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.422991  [11520/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.479526  [11584/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.406173  [11648/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.378248  [11712/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.389148  [11776/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.427748  [11840/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.394384  [11904/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.371671  [11968/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.438657  [12032/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.476690  [12096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.377134  [12160/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.452976  [12224/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.432754  [12288/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.421513  [12352/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.450702  [12416/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.344377  [12480/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.285094  [12544/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.392278  [12608/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.371395  [12672/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.363722  [12736/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.430649  [12800/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.413412  [12864/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.362742  [12928/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.333746  [12992/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.268526  [13056/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.397562  [13120/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.414967  [13184/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.371341  [13248/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.332647  [13312/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.483464  [13376/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.373283  [13440/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.314548  [13504/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.278691  [13568/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.421452  [13632/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.480636  [13696/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.341386  [13760/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.400087  [13824/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.480064  [13888/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.461017  [13952/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.375226  [14016/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.541372  [14080/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.362203  [14144/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.285742  [14208/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.372164  [14272/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.536796  [14336/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.255718  [14400/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.482785  [14464/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.368250  [14528/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.406805  [14592/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.507036  [14656/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.363905  [14720/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.487057  [14784/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.388768  [14848/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.393507  [14912/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.296224  [14976/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.400213  [15040/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.367190  [15104/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.305978  [15168/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.504032  [15232/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.516619  [15296/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.331777  [15360/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.397823  [15424/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.475583  [15488/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.345370  [15552/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.400867  [15616/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.444442  [15680/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.334739  [15744/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.288459  [15808/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.346581  [15872/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.289231  [15936/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.353601  [16000/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.341381  [16064/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.399225  [16128/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.434024  [16192/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.305570  [16256/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.315718  [16320/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.410138  [16384/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.436468  [16448/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.371364  [16512/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.313746  [16576/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.408773  [16640/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.382360  [16704/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.328259  [16768/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.243799  [16832/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.392266  [16896/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.438845  [16960/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.386988  [17024/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.365035  [17088/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.303697  [17152/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.232564  [17216/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.270937  [17280/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.534844  [17344/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.416051  [17408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.427381  [17472/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.383843  [17536/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.451810  [17600/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.507435  [17664/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.405249  [17728/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.519329  [17792/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.331584  [17856/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.379505  [17920/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.471786  [17984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.371893  [18048/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.368710  [18112/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.424278  [18176/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.477590  [18240/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.299561  [18304/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.497022  [18368/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.328865  [18432/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.426360  [18496/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.329625  [18560/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.376467  [18624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.271327  [18688/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.359749  [18752/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.272072  [18816/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.285570  [18880/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.451461  [18944/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.414822  [19008/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.340563  [19072/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.388755  [19136/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.255038  [19200/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.400194  [19264/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.466436  [19328/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.460709  [19392/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.475870  [19456/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.337964  [19520/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.342593  [19584/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.266826  [19648/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.389594  [19712/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.395244  [19776/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.342209  [19840/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.549921  [19904/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.548145  [19968/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.292385  [20032/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.385315  [20096/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.264170  [20160/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.393540  [20224/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.288282  [20288/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.316127  [20352/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.495828  [20416/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.272368  [20480/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.651272  [20544/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.447234  [20608/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.370323  [20672/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.464492  [20736/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.442429  [20800/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.380473  [20864/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.383088  [20928/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.386709  [20992/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.504403  [21056/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.338491  [21120/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.337561  [21184/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.320929  [21248/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.320324  [21312/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.316341  [11690/21347]\n",
      "training accuracy: 82.85714387893677%\n",
      "validation accuracy: 96.875%\n",
      "---------------\n",
      "Epoch 13\n",
      "---------------\n",
      "loss: 0.470270  [   64/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.254789  [  128/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.382301  [  192/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.388682  [  256/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.515319  [  320/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.477800  [  384/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.520443  [  448/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.366322  [  512/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.491878  [  576/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.386056  [  640/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.399196  [  704/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.466248  [  768/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.426954  [  832/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.351792  [  896/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.415235  [  960/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.409482  [ 1024/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.454288  [ 1088/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.361759  [ 1152/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.481276  [ 1216/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.362042  [ 1280/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.375826  [ 1344/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.525477  [ 1408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.312580  [ 1472/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.365131  [ 1536/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.402264  [ 1600/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.323825  [ 1664/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.526491  [ 1728/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.311889  [ 1792/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.272266  [ 1856/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.302299  [ 1920/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.451975  [ 1984/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.306311  [ 2048/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.361755  [ 2112/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.369287  [ 2176/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.356338  [ 2240/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.471270  [ 2304/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.313975  [ 2368/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.330000  [ 2432/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.459459  [ 2496/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.441861  [ 2560/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.451154  [ 2624/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.344169  [ 2688/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.377555  [ 2752/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.326800  [ 2816/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.261714  [ 2880/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.363250  [ 2944/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.498193  [ 3008/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.416732  [ 3072/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.402392  [ 3136/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.361256  [ 3200/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.384229  [ 3264/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.397823  [ 3328/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.366642  [ 3392/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.275261  [ 3456/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.415217  [ 3520/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.261759  [ 3584/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.429052  [ 3648/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.322720  [ 3712/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.394189  [ 3776/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.402073  [ 3840/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.294561  [ 3904/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.352187  [ 3968/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.461186  [ 4032/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.467070  [ 4096/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.480380  [ 4160/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.502400  [ 4224/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.368806  [ 4288/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.466641  [ 4352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.338341  [ 4416/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.416892  [ 4480/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.341716  [ 4544/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.312685  [ 4608/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.316155  [ 4672/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.355629  [ 4736/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.385940  [ 4800/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.391566  [ 4864/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.430419  [ 4928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.306269  [ 4992/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.404032  [ 5056/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.451257  [ 5120/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.401884  [ 5184/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.338708  [ 5248/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.375701  [ 5312/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.430194  [ 5376/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.414330  [ 5440/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.270276  [ 5504/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.452189  [ 5568/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.400117  [ 5632/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.382938  [ 5696/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.352479  [ 5760/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.289245  [ 5824/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.339576  [ 5888/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.336454  [ 5952/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.372110  [ 6016/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.459352  [ 6080/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.548419  [ 6144/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.320960  [ 6208/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.423212  [ 6272/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.381811  [ 6336/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.494011  [ 6400/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.372791  [ 6464/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.377317  [ 6528/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.334048  [ 6592/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.414076  [ 6656/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.436630  [ 6720/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.424671  [ 6784/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.394057  [ 6848/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.466143  [ 6912/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.332719  [ 6976/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.421316  [ 7040/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.334639  [ 7104/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.358353  [ 7168/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.531060  [ 7232/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.379002  [ 7296/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.356532  [ 7360/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.452177  [ 7424/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.470732  [ 7488/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.479374  [ 7552/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.269253  [ 7616/21347]\n",
      "training accuracy: 95.3125%\n",
      "loss: 0.395749  [ 7680/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.458450  [ 7744/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.337652  [ 7808/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.357300  [ 7872/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.369122  [ 7936/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.549233  [ 8000/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.389720  [ 8064/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.502019  [ 8128/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.514796  [ 8192/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.503081  [ 8256/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.309522  [ 8320/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.290644  [ 8384/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.334512  [ 8448/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.338268  [ 8512/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.338196  [ 8576/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.337210  [ 8640/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.385269  [ 8704/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.419962  [ 8768/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.307991  [ 8832/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.390586  [ 8896/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.466178  [ 8960/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.466225  [ 9024/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.442963  [ 9088/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.417033  [ 9152/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.315509  [ 9216/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.234408  [ 9280/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.287119  [ 9344/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.433512  [ 9408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.354057  [ 9472/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.380693  [ 9536/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.381577  [ 9600/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.338325  [ 9664/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.376216  [ 9728/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.503294  [ 9792/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.369325  [ 9856/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.329769  [ 9920/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.376250  [ 9984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.451208  [10048/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.356618  [10112/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.348897  [10176/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.318483  [10240/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.367824  [10304/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.353520  [10368/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.350974  [10432/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.405112  [10496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.434769  [10560/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.335292  [10624/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.391445  [10688/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.331156  [10752/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.472146  [10816/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.373154  [10880/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.502586  [10944/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.322404  [11008/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.265934  [11072/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.449110  [11136/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.394988  [11200/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.295742  [11264/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.375975  [11328/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.423924  [11392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.353355  [11456/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.437137  [11520/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.348538  [11584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.310432  [11648/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.263184  [11712/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.334173  [11776/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.383360  [11840/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.391437  [11904/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.393889  [11968/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.458271  [12032/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.381985  [12096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.482405  [12160/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.453999  [12224/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.351182  [12288/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.368472  [12352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.425511  [12416/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.440559  [12480/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.415760  [12544/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.441369  [12608/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.327409  [12672/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.374041  [12736/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.490005  [12800/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.403910  [12864/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.502369  [12928/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.466801  [12992/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.512676  [13056/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.313316  [13120/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.444319  [13184/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.343776  [13248/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.376701  [13312/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.395351  [13376/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.389211  [13440/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.366532  [13504/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.439647  [13568/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.350546  [13632/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.344457  [13696/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.415395  [13760/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.295132  [13824/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.431115  [13888/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.366242  [13952/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.457549  [14016/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.336568  [14080/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.419772  [14144/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.287938  [14208/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.354816  [14272/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.343481  [14336/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.469923  [14400/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.313072  [14464/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.385070  [14528/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.363016  [14592/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.291631  [14656/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.296001  [14720/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.310909  [14784/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.390210  [14848/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.591589  [14912/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.370200  [14976/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.361713  [15040/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.415485  [15104/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.457548  [15168/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.398759  [15232/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.562808  [15296/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.436417  [15360/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.393977  [15424/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.400266  [15488/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.390426  [15552/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.512888  [15616/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.437299  [15680/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.405009  [15744/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.248790  [15808/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.474108  [15872/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.361244  [15936/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.823288  [16000/21347]\n",
      "training accuracy: 67.1875%\n",
      "loss: 0.343339  [16064/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.408175  [16128/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.314057  [16192/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.450553  [16256/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.300552  [16320/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.328869  [16384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.337289  [16448/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.373949  [16512/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.350532  [16576/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.444117  [16640/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.470870  [16704/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.407163  [16768/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.334980  [16832/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.445756  [16896/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.380665  [16960/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.265887  [17024/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.435853  [17088/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.497305  [17152/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.368248  [17216/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.279676  [17280/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.419537  [17344/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.460403  [17408/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.492119  [17472/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.386694  [17536/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.354180  [17600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.274102  [17664/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.384495  [17728/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.378654  [17792/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.431881  [17856/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.321508  [17920/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.324011  [17984/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.326120  [18048/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.409310  [18112/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.287935  [18176/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.370848  [18240/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.488771  [18304/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.351765  [18368/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.358066  [18432/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.356866  [18496/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.403432  [18560/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.302305  [18624/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.348398  [18688/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.492381  [18752/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.503338  [18816/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.395540  [18880/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.309575  [18944/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.329010  [19008/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.461757  [19072/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.413787  [19136/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.361229  [19200/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.555978  [19264/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.373397  [19328/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.311150  [19392/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.415402  [19456/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.365747  [19520/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.298115  [19584/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.343384  [19648/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.398061  [19712/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.367623  [19776/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.359511  [19840/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.445213  [19904/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.336295  [19968/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.379740  [20032/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.312851  [20096/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.354966  [20160/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.413993  [20224/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.417942  [20288/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.418037  [20352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.397217  [20416/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.456688  [20480/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.373400  [20544/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.315656  [20608/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.354119  [20672/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.353355  [20736/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.402212  [20800/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.423259  [20864/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.362501  [20928/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.328149  [20992/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.262255  [21056/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.335763  [21120/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.286698  [21184/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.351086  [21248/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.305923  [21312/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.422588  [11690/21347]\n",
      "training accuracy: 85.71428656578064%\n",
      "validation accuracy: 96.875%\n",
      "---------------\n",
      "Epoch 14\n",
      "---------------\n",
      "loss: 0.396385  [   64/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.290751  [  128/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.268476  [  192/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.376607  [  256/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.335506  [  320/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.359175  [  384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.260945  [  448/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.359789  [  512/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.288020  [  576/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.530279  [  640/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.333024  [  704/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.436648  [  768/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.407087  [  832/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.390160  [  896/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.651362  [  960/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.438192  [ 1024/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.314977  [ 1088/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.316651  [ 1152/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.289181  [ 1216/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.447904  [ 1280/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.508796  [ 1344/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.375580  [ 1408/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.401524  [ 1472/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.366077  [ 1536/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.397023  [ 1600/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.354540  [ 1664/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.392038  [ 1728/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.360289  [ 1792/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.437297  [ 1856/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.366736  [ 1920/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.336245  [ 1984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.427677  [ 2048/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.404935  [ 2112/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.475133  [ 2176/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.284392  [ 2240/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.270952  [ 2304/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.356064  [ 2368/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.325954  [ 2432/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.336208  [ 2496/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.354778  [ 2560/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.459750  [ 2624/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.343486  [ 2688/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.503430  [ 2752/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.303178  [ 2816/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.420853  [ 2880/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.420387  [ 2944/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.325045  [ 3008/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.385201  [ 3072/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.333977  [ 3136/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.337645  [ 3200/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.317891  [ 3264/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.364275  [ 3328/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.367678  [ 3392/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.366124  [ 3456/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.364650  [ 3520/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.335379  [ 3584/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.468351  [ 3648/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.539164  [ 3712/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.470910  [ 3776/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.269100  [ 3840/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.418384  [ 3904/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.291221  [ 3968/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.382680  [ 4032/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.447132  [ 4096/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.380229  [ 4160/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.374877  [ 4224/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.326553  [ 4288/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.461020  [ 4352/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.276116  [ 4416/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.305088  [ 4480/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.247273  [ 4544/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.366797  [ 4608/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.440834  [ 4672/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.371990  [ 4736/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.383320  [ 4800/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.362693  [ 4864/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.453642  [ 4928/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.372859  [ 4992/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.290662  [ 5056/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.383059  [ 5120/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.280887  [ 5184/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.302088  [ 5248/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.555208  [ 5312/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.305711  [ 5376/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.352037  [ 5440/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.363401  [ 5504/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.291357  [ 5568/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.324905  [ 5632/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.393961  [ 5696/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.373308  [ 5760/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.242143  [ 5824/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.454344  [ 5888/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.320655  [ 5952/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.424514  [ 6016/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.329498  [ 6080/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.269478  [ 6144/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.422068  [ 6208/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.474792  [ 6272/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.264693  [ 6336/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.321885  [ 6400/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.394852  [ 6464/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.527654  [ 6528/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.381926  [ 6592/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.277178  [ 6656/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.332476  [ 6720/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.256281  [ 6784/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.333032  [ 6848/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.320940  [ 6912/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.355402  [ 6976/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.315638  [ 7040/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.500297  [ 7104/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.409838  [ 7168/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.428488  [ 7232/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.281790  [ 7296/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.373098  [ 7360/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.383760  [ 7424/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.443749  [ 7488/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.464989  [ 7552/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.354220  [ 7616/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.362638  [ 7680/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.361810  [ 7744/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.421376  [ 7808/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.419434  [ 7872/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.370235  [ 7936/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.448354  [ 8000/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.374391  [ 8064/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.327275  [ 8128/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.299172  [ 8192/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.483395  [ 8256/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.519378  [ 8320/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.557393  [ 8384/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.319737  [ 8448/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.305550  [ 8512/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.414471  [ 8576/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.441164  [ 8640/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.462501  [ 8704/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.350929  [ 8768/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.342383  [ 8832/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.537710  [ 8896/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.367345  [ 8960/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.388313  [ 9024/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.552824  [ 9088/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.394765  [ 9152/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.417092  [ 9216/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.407633  [ 9280/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.498389  [ 9344/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.461259  [ 9408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.358392  [ 9472/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.456792  [ 9536/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.436013  [ 9600/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.309966  [ 9664/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.428215  [ 9728/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.373826  [ 9792/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.390780  [ 9856/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.283208  [ 9920/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.504168  [ 9984/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.402894  [10048/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.492577  [10112/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.393596  [10176/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.262082  [10240/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.464067  [10304/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.293081  [10368/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.515707  [10432/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.473569  [10496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.426916  [10560/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.365322  [10624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.432680  [10688/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.444115  [10752/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.422457  [10816/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.390436  [10880/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.281566  [10944/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.283796  [11008/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.437959  [11072/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.398088  [11136/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.236465  [11200/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.318542  [11264/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.487088  [11328/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.394225  [11392/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.573056  [11456/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.415502  [11520/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.414550  [11584/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.375832  [11648/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.292190  [11712/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.428232  [11776/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.356301  [11840/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.433076  [11904/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.451106  [11968/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.381860  [12032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.454631  [12096/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.349287  [12160/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.408381  [12224/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.327250  [12288/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.345997  [12352/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.384042  [12416/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.348972  [12480/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.389148  [12544/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.353834  [12608/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.341940  [12672/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.367230  [12736/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.272518  [12800/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.382695  [12864/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.360049  [12928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.312719  [12992/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.377155  [13056/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.423046  [13120/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.368335  [13184/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.428340  [13248/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.454203  [13312/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.421044  [13376/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.433291  [13440/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.263467  [13504/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.361802  [13568/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.362929  [13632/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.348265  [13696/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.443034  [13760/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.407404  [13824/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.417495  [13888/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.456838  [13952/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.353811  [14016/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.428830  [14080/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.398222  [14144/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.371718  [14208/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.441524  [14272/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.344705  [14336/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.429933  [14400/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.541835  [14464/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.492692  [14528/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.309214  [14592/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.356289  [14656/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.449231  [14720/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.384267  [14784/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.503420  [14848/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.422182  [14912/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.413708  [14976/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.381553  [15040/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.325358  [15104/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.428050  [15168/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.410505  [15232/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.522050  [15296/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.390130  [15360/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.487371  [15424/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.407888  [15488/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.352991  [15552/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.429338  [15616/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.384652  [15680/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.463286  [15744/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.347200  [15808/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.473688  [15872/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.371045  [15936/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.366948  [16000/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.320183  [16064/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.355723  [16128/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.298731  [16192/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.413357  [16256/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.444844  [16320/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.507836  [16384/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.449193  [16448/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.361957  [16512/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.431564  [16576/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.413458  [16640/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.358106  [16704/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.319009  [16768/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.318859  [16832/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.545768  [16896/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.318226  [16960/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.358750  [17024/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.328226  [17088/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.269925  [17152/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.271170  [17216/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.398862  [17280/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.419235  [17344/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.673927  [17408/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.496319  [17472/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.313518  [17536/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.403519  [17600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.404790  [17664/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.360192  [17728/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.374716  [17792/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.439574  [17856/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.402618  [17920/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.425998  [17984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.344583  [18048/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.432217  [18112/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.337061  [18176/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.347590  [18240/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.473291  [18304/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.398320  [18368/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.340737  [18432/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.338935  [18496/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.364396  [18560/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.377966  [18624/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.318663  [18688/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.331976  [18752/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.357761  [18816/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.544901  [18880/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.431733  [18944/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.359370  [19008/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.365783  [19072/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.337867  [19136/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.409691  [19200/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.314322  [19264/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.423882  [19328/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.364493  [19392/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.359000  [19456/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.292767  [19520/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.326105  [19584/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.353497  [19648/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.523087  [19712/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.246189  [19776/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.431246  [19840/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.381076  [19904/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.293799  [19968/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.358013  [20032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.468997  [20096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.385169  [20160/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.382617  [20224/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.429465  [20288/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.392033  [20352/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.337935  [20416/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.400535  [20480/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.366344  [20544/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.342790  [20608/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.407389  [20672/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.363587  [20736/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.350455  [20800/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.374816  [20864/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.358989  [20928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.318646  [20992/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.374191  [21056/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.499918  [21120/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.430101  [21184/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.308878  [21248/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.362734  [21312/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.371799  [11690/21347]\n",
      "training accuracy: 82.85714387893677%\n",
      "validation accuracy: 85.9375%\n",
      "---------------\n",
      "Epoch 15\n",
      "---------------\n",
      "loss: 0.510782  [   64/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.378923  [  128/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.315117  [  192/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.278749  [  256/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.288908  [  320/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.313108  [  384/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.355783  [  448/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.542616  [  512/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.608392  [  576/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.512026  [  640/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.374667  [  704/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.341446  [  768/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.418848  [  832/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.413043  [  896/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.400918  [  960/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.392720  [ 1024/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.381498  [ 1088/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.409546  [ 1152/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.431320  [ 1216/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.347572  [ 1280/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.286243  [ 1344/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.554432  [ 1408/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.489090  [ 1472/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.415423  [ 1536/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.404149  [ 1600/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.362026  [ 1664/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.440177  [ 1728/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.344950  [ 1792/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.411886  [ 1856/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.432519  [ 1920/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.342143  [ 1984/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.417875  [ 2048/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.246905  [ 2112/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.338751  [ 2176/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.314325  [ 2240/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.382798  [ 2304/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.448273  [ 2368/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.318457  [ 2432/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.373229  [ 2496/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.337144  [ 2560/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.485629  [ 2624/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.316448  [ 2688/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.470443  [ 2752/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.362373  [ 2816/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.431273  [ 2880/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.536063  [ 2944/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.362076  [ 3008/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.453246  [ 3072/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.362077  [ 3136/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.365613  [ 3200/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.393417  [ 3264/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.422692  [ 3328/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.424634  [ 3392/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.285408  [ 3456/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.467222  [ 3520/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.345705  [ 3584/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.476838  [ 3648/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.414431  [ 3712/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.487764  [ 3776/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.214114  [ 3840/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.301320  [ 3904/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.285513  [ 3968/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.413847  [ 4032/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.454043  [ 4096/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.367003  [ 4160/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.331692  [ 4224/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.401469  [ 4288/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.367199  [ 4352/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.412306  [ 4416/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.446478  [ 4480/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.370389  [ 4544/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.475872  [ 4608/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.408567  [ 4672/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.422577  [ 4736/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.449876  [ 4800/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.385881  [ 4864/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.436440  [ 4928/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.485396  [ 4992/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.405634  [ 5056/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.316315  [ 5120/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.368426  [ 5184/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.329728  [ 5248/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.344921  [ 5312/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.303251  [ 5376/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.334146  [ 5440/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.285569  [ 5504/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.223070  [ 5568/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.506268  [ 5632/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.500340  [ 5696/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.278956  [ 5760/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.391810  [ 5824/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.326906  [ 5888/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.461677  [ 5952/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.271688  [ 6016/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.305152  [ 6080/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.377681  [ 6144/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.389126  [ 6208/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.435647  [ 6272/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.301290  [ 6336/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.289529  [ 6400/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.310237  [ 6464/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.544059  [ 6528/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.329740  [ 6592/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.358612  [ 6656/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.295802  [ 6720/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.402459  [ 6784/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.554101  [ 6848/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.246200  [ 6912/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.404445  [ 6976/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.360013  [ 7040/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.363785  [ 7104/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.388007  [ 7168/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.332973  [ 7232/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.290482  [ 7296/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.233497  [ 7360/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.395625  [ 7424/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.468291  [ 7488/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.466917  [ 7552/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.442196  [ 7616/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.363030  [ 7680/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.324030  [ 7744/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.444801  [ 7808/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.380425  [ 7872/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.325095  [ 7936/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.347725  [ 8000/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.405941  [ 8064/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.446290  [ 8128/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.456238  [ 8192/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.296622  [ 8256/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.303040  [ 8320/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.395078  [ 8384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.460928  [ 8448/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.376786  [ 8512/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.348638  [ 8576/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.324668  [ 8640/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.356883  [ 8704/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.307041  [ 8768/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.358804  [ 8832/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.449469  [ 8896/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.425166  [ 8960/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.406522  [ 9024/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.480323  [ 9088/21347]\n",
      "training accuracy: 70.3125%\n",
      "loss: 0.352339  [ 9152/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.293741  [ 9216/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.375331  [ 9280/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.402016  [ 9344/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.467068  [ 9408/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.299102  [ 9472/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.352316  [ 9536/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.338197  [ 9600/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.333459  [ 9664/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.423417  [ 9728/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.349068  [ 9792/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.375290  [ 9856/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.473106  [ 9920/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.362968  [ 9984/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.449911  [10048/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.284741  [10112/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.404660  [10176/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.437074  [10240/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.421807  [10304/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.387214  [10368/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.335946  [10432/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.429815  [10496/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.397933  [10560/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.392636  [10624/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.297445  [10688/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.316194  [10752/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.265261  [10816/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.491588  [10880/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.291632  [10944/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.362417  [11008/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.431170  [11072/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.304192  [11136/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.657556  [11200/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.292306  [11264/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.392973  [11328/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.471295  [11392/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.301721  [11456/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.352993  [11520/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.405344  [11584/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.287559  [11648/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.301712  [11712/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.432293  [11776/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.364182  [11840/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.401054  [11904/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.355464  [11968/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.374988  [12032/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.452140  [12096/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.484143  [12160/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.281556  [12224/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.393921  [12288/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.396623  [12352/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.401749  [12416/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.549561  [12480/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.401861  [12544/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.372405  [12608/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.362770  [12672/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.449218  [12736/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.373640  [12800/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.366675  [12864/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.287217  [12928/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.464155  [12992/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.322178  [13056/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.361338  [13120/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.386866  [13184/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.363116  [13248/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.359798  [13312/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.461490  [13376/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.318234  [13440/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.415709  [13504/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.500684  [13568/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.403347  [13632/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.288477  [13696/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.355290  [13760/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.354477  [13824/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.378503  [13888/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.342275  [13952/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.366317  [14016/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.402405  [14080/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.372903  [14144/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.329879  [14208/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.393819  [14272/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.332274  [14336/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.380478  [14400/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.513710  [14464/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.437596  [14528/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.400565  [14592/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.289394  [14656/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.366546  [14720/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.361061  [14784/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.356190  [14848/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.314471  [14912/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.426435  [14976/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.408486  [15040/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.357860  [15104/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.411826  [15168/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.346241  [15232/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.421475  [15296/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.367534  [15360/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.365385  [15424/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.479882  [15488/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.375432  [15552/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.316250  [15616/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.415452  [15680/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.497449  [15744/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.471817  [15808/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.402747  [15872/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.461658  [15936/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.408900  [16000/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.307188  [16064/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.383342  [16128/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.512553  [16192/21347]\n",
      "training accuracy: 68.75%\n",
      "loss: 0.448150  [16256/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.389627  [16320/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.443848  [16384/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.393503  [16448/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.356266  [16512/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.327246  [16576/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.374478  [16640/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.401655  [16704/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.237703  [16768/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.319815  [16832/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.404193  [16896/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.395117  [16960/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.317227  [17024/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.405133  [17088/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.400053  [17152/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.528665  [17216/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.342028  [17280/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.414715  [17344/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.409468  [17408/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.460345  [17472/21347]\n",
      "training accuracy: 73.4375%\n",
      "loss: 0.252569  [17536/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.371824  [17600/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.329241  [17664/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.589108  [17728/21347]\n",
      "training accuracy: 60.9375%\n",
      "loss: 0.396482  [17792/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.439539  [17856/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.269604  [17920/21347]\n",
      "training accuracy: 89.0625%\n",
      "loss: 0.456611  [17984/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.448138  [18048/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.371706  [18112/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.275614  [18176/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.326283  [18240/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.324771  [18304/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.420079  [18368/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.302466  [18432/21347]\n",
      "training accuracy: 90.625%\n",
      "loss: 0.306989  [18496/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.399937  [18560/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.283217  [18624/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.383448  [18688/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.513444  [18752/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.406448  [18816/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.395378  [18880/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.332045  [18944/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.360258  [19008/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.376431  [19072/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.344009  [19136/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.299282  [19200/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.385951  [19264/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.352313  [19328/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.354900  [19392/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.388694  [19456/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.262127  [19520/21347]\n",
      "training accuracy: 93.75%\n",
      "loss: 0.470112  [19584/21347]\n",
      "training accuracy: 79.6875%\n",
      "loss: 0.400089  [19648/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.193130  [19712/21347]\n",
      "training accuracy: 92.1875%\n",
      "loss: 0.494512  [19776/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.569317  [19840/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.358201  [19904/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.531782  [19968/21347]\n",
      "training accuracy: 71.875%\n",
      "loss: 0.392072  [20032/21347]\n",
      "training accuracy: 76.5625%\n",
      "loss: 0.317863  [20096/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.376462  [20160/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.337489  [20224/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.330499  [20288/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.347486  [20352/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.294716  [20416/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.338124  [20480/21347]\n",
      "training accuracy: 87.5%\n",
      "loss: 0.525721  [20544/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.339726  [20608/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.473403  [20672/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.390528  [20736/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.356969  [20800/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.455278  [20864/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.363123  [20928/21347]\n",
      "training accuracy: 82.8125%\n",
      "loss: 0.367227  [20992/21347]\n",
      "training accuracy: 78.125%\n",
      "loss: 0.391933  [21056/21347]\n",
      "training accuracy: 75.0%\n",
      "loss: 0.357066  [21120/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.342714  [21184/21347]\n",
      "training accuracy: 84.375%\n",
      "loss: 0.306443  [21248/21347]\n",
      "training accuracy: 85.9375%\n",
      "loss: 0.419917  [21312/21347]\n",
      "training accuracy: 81.25%\n",
      "loss: 0.257971  [11690/21347]\n",
      "training accuracy: 88.57142925262451%\n",
      "validation accuracy: 93.75%\n",
      "---------------\n",
      "Early stopped training at epoch 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Using {device}')\n",
    "print(f\"There are {len(train_dataset)} images in training set and {len(val_dataset)} images in validation set\\n\")\n",
    "val_iter = iter(val_loader)\n",
    "train_val(train_loader, val_iter, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3116.674484,
   "end_time": "2023-10-12T10:27:28.364077",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-12T09:35:31.689593",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "091ed620396f48969b30f8379487f503": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d39a71aef92d471a98e769dbf4b40a6c",
       "placeholder": "​",
       "style": "IPY_MODEL_aaf7ba5ed9d441ecb9825f98a9857a15",
       "value": "100%"
      }
     },
     "2feae7cb226b4e59bf7cf4f4be2c5165": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4afc7a20e2f6457092ff4428f7bc142f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b00ae9c6cec4a74a2339a20844505da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_608960e6b1f844cf8dd3e66a1c54f625",
       "max": 26684.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2feae7cb226b4e59bf7cf4f4be2c5165",
       "value": 26684.0
      }
     },
     "5f5a26be52ff4ebe8b5c4b661e2700cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_091ed620396f48969b30f8379487f503",
        "IPY_MODEL_5b00ae9c6cec4a74a2339a20844505da",
        "IPY_MODEL_b3d3718cbdb54541a8e7984d2737baed"
       ],
       "layout": "IPY_MODEL_4afc7a20e2f6457092ff4428f7bc142f"
      }
     },
     "608960e6b1f844cf8dd3e66a1c54f625": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ee9f0757c784bf0a78593b79a549034": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d4efe05021f468ea7653fb2faf5cf5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "aaf7ba5ed9d441ecb9825f98a9857a15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b3d3718cbdb54541a8e7984d2737baed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6ee9f0757c784bf0a78593b79a549034",
       "placeholder": "​",
       "style": "IPY_MODEL_8d4efe05021f468ea7653fb2faf5cf5a",
       "value": " 26684/26684 [06:11&lt;00:00, 75.38it/s]"
      }
     },
     "d39a71aef92d471a98e769dbf4b40a6c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
